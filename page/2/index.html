<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SanYuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SanYuan">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="SanYuan">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SanYuan">
  
    <link rel="alternate" href="/atom.xml" title="SanYuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">SanYuan</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-前言" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/15/前言/" class="article-date">
  <time datetime="2016-12-15T00:23:06.000Z" itemprop="datePublished">2016-12-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/15/前言/">前言</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/15/前言/" data-id="ciwps8iov002850cye7ftbbaz" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Ubuntu-12-04-Puppet" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/15/Ubuntu-12-04-Puppet/" class="article-date">
  <time datetime="2016-12-15T00:09:45.000Z" itemprop="datePublished">2016-12-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/15/Ubuntu-12-04-Puppet/">Ubuntu 12.04 Puppet</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前言<br>系统运维本是一件枯燥，而且重复性强的工作，不停地搞命令占据了运维人员的大量时间。Puppet通过一种集中式管理的设计，让运维变得简单可控。管理员只需要在master节点修改配置，绑定的客户端节点就会自动同步配置，让命令行也能通过配置实现。<br>第一次尝试，从安装开始。<br>目录<br>Puppet是什么？<br>Puppet服务器安装及配置<br>Puppet客户端安装及配置<br>注册puppet客户端到服务器端<br>例子：生成文件测试</p>
<ol>
<li>Puppet是什么？<br>puppet是一种Linux、Unix、windows平台的集中配置管理系统，使用自有的puppet描述语言，可管理配置文件、用户、cron任务、软件包、系统服务等。puppet把这些系统实体称之为资源，puppet的设计目标是简化对这些资源的管理以及妥善处理资源间的依赖关系。<br>puppet采用C/S星状的结构，所有的客户端和一个或几个服务器交互。每个客户端周期的（默认半个小时）向服务器发送请求，获得其最新的配置信息，保证和该配置信息同步。每个puppet客户端每半小时(可以设置)连接一次服务器端, 下载最新的配置文件,并且严格按照配置文件来配置服务器. 配置完成以后,puppet客户端可以反馈给服务器端一个消息. 如果出错,也会给服务器端反馈一个消息.<br>2台服务器的环境配置：<br>Master Server：<br>Linux Ubuntu 12.04.2 LTS 64bit<br>ip: 192.168.1.201<br>hostname: vm1<br>ruby: 1.8.7<br>Client Server：<br>Linux Ubuntu 12.04.2 LTS 64bit<br>ip: 192.168.1.202<br>hostname: vm2<br>ruby: 1.8.7</li>
<li>Puppet服务器安装及配置<br>切换到root用户<br>设置host<br>安装Puppet服务器端<br>切换到root用户<br>~ sudo -i<br>~ whoami<br>root<br>设置host<br>~ hostname vm1<br>~ hostanme<br>vm1<br>~ vi /etc/hosts<br>127.0.0.1       localhost<br>192.168.1.201   vm1<br>192.168.1.202   vm2<br>2). 安装Puppet服务器端<br>#安装并启动Puppet服务器端<br>~ apt-get install puppetmaster<br>#检查端口8140<br>~ netstat -nlt |grep 8140<br>tcp        0      0 0.0.0.0:8140            0.0.0.0:*               LISTEN<br>#查看Puppet的版本<br>~ puppet -V<br>2.7.11<br>#启动Puppet服务器<br>~  puppet master –verbose –no-daemonize<br>notice: Starting Puppet master version 2.7.11<br>3). 配置Puppet服务器，在[master]标签下，增加certname设置<br>~ vi /etc/puppet/puppet.conf<br>[master]<br>certname=vm1 #增加服务器配置<br>#重启Puppet服务器<br>~ /etc/init.d/puppetmaster restart</li>
</ol>
<ul>
<li>Restarting puppet master<br>4). 查看Puppet服务器的证书<br>~ puppet cert –list –all</li>
</ul>
<ul>
<li>“vm1” (60:0E:1D:8F:80:0B:5D:7D:F8:8B:C7:C6:DF:CF:69:43)</li>
<li>为已注册的证书<br>这样我们就完成了，服务器端的配置！接下来，继续进行客户端的配置。</li>
</ul>
<ol>
<li>Puppet客户端安装及配置<br>切换到root用户<br>~ sudo -i<br>~ whoami<br>root<br>设置host<br>~ hostname vm2<br>~ hostanme<br>vm2<br>~ vi /etc/hosts<br>127.0.0.1       localhost<br>192.168.1.201   vm1<br>192.168.1.202   vm2<br>2). 安装Puppet服务器端<br>#安装并启动Puppet客户端<br>~ apt-get install puppet<br>#查看Puppet的版本<br>~ puppet -V<br>2.7.11<br>3). 配置Puppet客户端，在[agent]标签下增加server设置<br>~ sudo vi /etc/puppet/puppet.conf<br>[agent]<br>server=vm1<br>4). 配置Puppet客户端default值，修改START设置<br>~ vi /etc/default/puppet<br>START=yes<br>设置START把no改为yes<br>重启Puppet客户端<br>~ /etc/init.d/puppet restart</li>
</ol>
<ul>
<li>Restarting puppet agent</li>
</ul>
<ol>
<li>注册puppet客户端到服务器端<br>1). 检查服务器端的证书<br>#服务器端操作<br>~ puppet cert –list –all</li>
</ol>
<ul>
<li>“vm1” (60:0E:1D:8F:80:0B:5D:7D:F8:8B:C7:C6:DF:CF:69:43)<br>2). 在服务器端(vm1)创建文件site.pp<br>#服务器端操作<br>~ sudo vi /etc/puppet/manifests/site.pp<br>node default {<br>notify { “Hey ! It works !”: }<br>}<br>3). 从客户端(vm2)向服务器端发起注册请求<br>#客户端操作<br>~ puppet agent -t<br>info: Creating a new SSL key for vm2<br>info: Caching certificate for ca<br>info: Creating a new SSL certificate request for vm2<br>info: Certificate Request fingerprint (md5): 15:45:D1:22:1B:97:6D:49:43:BC:93:D8:BB:4C:E1:99<br>Exiting; no certificate found and waitforcert is disabled<br>#如果之前生成发启过认证请求，则需要删除证书的目录<br>~ rm -rf /var/lib/puppet/ssl<br>~ puppet agent -t<br>4). 服务器端(vm1)查看客户端请求认证的列表<br>#服务器端操作<br>~ puppet cert –list<br>“vm2” (15:45:D1:22:1B:97:6D:49:43:BC:93:D8:BB:4C:E1:99)<br>5). 服务器端(vm1)接受客户端的请求<br>#服务器端操作<br>~ puppet cert –sign vm2<br>notice: Signed certificate request for vm2<br>notice: Removing file Puppet::SSL::CertificateRequest vm2 at ‘/var/lib/puppet/ssl/ca/requests/vm2.pem’<br>#可以接受所有的请求<br>~ puppet cert -s -a<br>#查看认证后，查看服务器上公钥的位置<br>~ sudo ls  /var/lib/puppet/ssl/ca/signed/<br>vm1.pem  vm2.pem<br>6). 服务器端(vm1)查看的证书列表<br>#服务器端操作<br>~ puppet cert –list –all</li>
<li>“vm1” (B7:45:28:1B:AE:53:0B:D7:38:8E:66:70:6C:B8:13:A4)</li>
<li>“vm2” (FB:64:C7:38:DA:B4:C7:0B:E6:3B:85:86:9A:F4:61:C2)<br>7). 客户端(vm2)执行服务器设置的脚本site.pp<br>#客户端操作<br>~ puppetd –test<br>info: Caching certificate for vm2<br>info: Caching certificate_revocation_list for ca<br>info: Caching catalog for vm2<br>info: Applying configuration version ‘1383527011’<br>notice: Hey ! It works !<br>notice: /Stage[main]//Node[default]/Notify[Hey ! It works !]/message: defined ‘message’ as ‘Hey ! It works !’<br>notice: Finished catalog run in 0.02 seconds<br>我们看到’Hey ! It works !’，被正常显示出来。</li>
</ul>
<ol>
<li>例子：生成文件测试<br>1). 生成文件测试，在/tmp目录下生成一个文件hello.txt。<br>在服务器端修改一个文件site.pp<br>~ vi /etc/puppet/manifests/hello.pp<br>node default {<br>notify { “Hey ! It works !”: }<br>file {<br> “/tmp/hello.txt”: content =&gt; “hello world”;<br>}<br>}<br>2). 在服务器端运行hello.pp文件<br>~ puppet apply /etc/puppet/manifests/site.pp<br>warning: Could not retrieve fact fqdn<br>warning: Host is missing hostname and/or domain: vm1<br>notice: /Stage[main]//Node[default]/File[/tmp/hello.txt]/ensure: defined content as ‘{md5}5eb63bbbe01eeed093cb22bb8f5acdc3’<br>notice: Hey ! It works !<br>notice: /Stage[main]//Node[default]/Notify[Hey ! It works !]/message: defined ‘message’ as ‘Hey ! It works !’<br>notice: Finished catalog run in 0.02 seconds<br>#检查生成的文件<br>~ cat /tmp/hello.txt<br>hello world<br>3). 在客户端同步<br>~  puppet agent –test –server=vm1<br>info: Caching catalog for vm2<br>info: Applying configuration version ‘1383528346’<br>notice: /Stage[main]//Node[default]/File[/tmp/hello.txt]/ensure: defined content as ‘{md5}5eb63bbbe01eeed093cb22bb8f5acdc3’<br>notice: Hey ! It works !<br>notice: /Stage[main]//Node[default]/Notify[Hey ! It works !]/message: defined ‘message’ as ‘Hey ! It works !’<br>notice: Finished catalog run in 0.07 seconds<br>#检查生成的文件<br>~ cat /tmp/hello.txt<br>hello world<br>这样我们就实现了第一个最简单的利用Puppet自动化运维的例子。出步尝试已经成功，接下来就像要深入研究Puppet到底能干什么，实现系统的高效运维。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/15/Ubuntu-12-04-Puppet/" data-id="ciwps8ing001150cyh5avjvvy" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-6-x-GoAccess" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-6-x-GoAccess/" class="article-date">
  <time datetime="2016-12-14T04:52:16.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-6-x-GoAccess/">CentOS 6.x GoAccess</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>前言<br>使用Nginx的网站可能会遇到访问流量异常、被友情检测、程序出现Bug等各种突然情况，这时大家的反应想必都是第一时间分析日志，然后发现日志有几十GB之多，又需要按照时间、错误类型或者关键字段检索信息时会不会有种醍醐灌顶、菊花一紧的错觉。文中介绍的方法不管是GoAccess还是sed/awk虽然可以解决一时的问题但未必能够治本，也许ELK(Logstash+ElasticSearch+Kibana)对我们大多数人来说是更合理的集中化日志管理解决方案。<br>日志固然重要，但努力建设适合业务发展的集中化日志管理平台才是基础核心<br>更新历史<br>2015年08月31日 - 增加Nginx日志按时间分割<br>2015年07月16日 - 初稿<br>阅读原文 - <a href="http://wsgzao.github.io/post/goaccess/" target="_blank" rel="external">http://wsgzao.github.io/post/goaccess/</a><br>扩展阅读<br>GoAccess - <a href="http://goaccess.io/" target="_blank" rel="external">http://goaccess.io/</a><br>用GoAccess分析Nginx的日志 - <a href="http://www.fancycoding.com/log-analyse-using-goaccess/" target="_blank" rel="external">http://www.fancycoding.com/log-analyse-using-goaccess/</a><br>sed 简明教程 - <a href="http://coolshell.cn/articles/9104.html" target="_blank" rel="external">http://coolshell.cn/articles/9104.html</a><br>AWK 简明教程 - <a href="http://coolshell.cn/articles/9070.html" target="_blank" rel="external">http://coolshell.cn/articles/9070.html</a><br>LTMP索引 - <a href="http://wsgzao.github.io/index/#LTMP" target="_blank" rel="external">http://wsgzao.github.io/index/#LTMP</a><br>安装GoAccess<br>各平台都有灰常简单的部署方案 - <a href="http://goaccess.io/download" target="_blank" rel="external">http://goaccess.io/download</a><br>wget <a href="http://tar.goaccess.io/goaccess-0.9.2.tar.gz" target="_blank" rel="external">http://tar.goaccess.io/goaccess-0.9.2.tar.gz</a><br>tar -xzvf goaccess-0.9.2.tar.gz<br>cd goaccess-0.9.2/<br>./configure –enable-utf8<br>make<br>make install<br>使用方式<br>更多常见问题请参考官方FAQ - <a href="http://goaccess.io/faq" target="_blank" rel="external">http://goaccess.io/faq</a></p>
<p>#直接打开<br>goaccess -f access.log</p>
<p>#选择日志格式<br>NCSA Combined Log Format</p>
<p>#剩下的操作都蛮简单的，参考扩展阅读和官方文档吧</p>
<p>#导出HTML报告会遇到的问题<br>goaccess -f time_access.log -a &gt; report.html</p>
<p>GoAccess - version 0.9.2 - Jul 15 2015 16:23:20<br>Config file: /usr/local/etc/goaccess.conf</p>
<p>Fatal error has occurred<br>Error occured at: src/parser.c - verify_formats - 1691<br>No time format was found on your conf file.</p>
<p>#添加配置文件<br>vi ~/.goaccessrc</p>
<p>time-format %T<br>date-format %d/%b/%Y<br>log-format %h %^[%d:%t %^] “%r” %s %b “%R” “%u”</p>
<p>#重新指定配置文件后执行<br>goaccess -f time_access.log -p ~/.goaccessrc -a &gt; report.html<br>使用bash/sed/awk手动查找Nginx日志<br>更多技巧可以参考扩展阅读，Python的处理效率或者更优</p>
<p>#按日期查找时间段<br>sed -n “/30\/Aug\/2015:00:00:01/,/30\/Aug\/2015:23:59:59/“p access.log &gt; time_access.log</p>
<p>#查找504错误的页面和数量<br>awk ‘($9 ~ /504/)’ time_access.log | awk ‘{print $7}’ | sort | uniq -c | sort -rn &gt; 504.log</p>
<p>#查找访问最多的20个IP及访问次数<br>awk ‘{print $1}’ time_access.log | sort | uniq -c | sort -n -k 1 -r | head -n 20 &gt; top.log<br>Nginx日志按时间分割<br>这个脚本可以作为通用模板，其它业务需求也可以参照</p>
<p>#增加自定义脚本<br>vi nginx_log.sh</p>
<p>#!/bin/bash</p>
<p>#设置crontab -e为每日凌晨3点</p>
<p>#0 3 <em> </em> * /root/OMCS/script/nginx_log.sh</p>
<p>for ngix_i in <code>ls /app/local</code><br>do</p>
<pre><code>#设置临时变量
nginx_dir=`basename ${ngix_i}`
if ( echo &quot;$nginx_dir&quot;|grep &quot;nginx&quot; &gt; /dev/null ); then
     # echo $nginx_dir
     #设置日志文件存放目录
     logs_path=&quot;/app/local/$nginx_dir/logs/&quot;
     #设置备份日志存放目录
     logs_bak=&quot;/app/local/$nginx_dir/logs/bak/&quot;
     #设置pid文件
     pid_path=&quot;/app/local/$nginx_dir/logs/nginx.pid&quot;

     #判断目录是否存在
     if  [ ! -d  &quot;$logs_path&quot; ]; then
          continue
     fi
     #判断文件是否存在
     if  [ ! -e  &quot;$pid_path&quot; ]; then
          continue
     fi

     #判断目录是否存在
     if  [ ! -d  &quot;$logs_bak&quot; ]; then
          mkdir -p &quot;$logs_bak&quot;
     fi

     cd ${logs_path}
     logfiles=(`ls *.log`)

     array_i=0
     #迁移日志循环模块
     for i in ${logfiles[@]};
     do
          #设置临时变量
          j=`basename ${i}`
          #重命名并迁移日志文件
          baklogname=${j}_$(date -d &quot;yesterday&quot; +&quot;%Y%m%d%H%M%S&quot;)
          baklognames[$array_i]=$baklogname
          mv ${i} ${logs_bak}${baklogname}
          array_i=`expr $array_i + 1`
     done

     #向nginx主进程发信号重新打开日志
     kill -USR1 `cat ${pid_path}`

     cd ${logs_bak}
     #压缩日志循环模块
     for i in ${baklognames[@]};
     do
          #压缩并删除原日志
          tar -zcvf ${i}.tar.gz ${i} --remove-files &gt; /dev/null 2&gt;&amp;1
     done

     #清理7天前的日志
     find ${logs_bak} -name &apos;*&apos;  -mtime +90 | xargs rm -rf {}

fi
</code></pre><p>done</p>
<p>#增加执行权限<br>chmod u+x nginx_log.sh</p>
<p>#设置crontab -e为每日凌晨3点<br>crontab -e</p>
<p>0 3 <em> </em> * /root/OMCS/script/nginx_log.sh</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-6-x-GoAccess/" data-id="ciwps8im4000750cywbrk9ksq" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-6-x-Logrotate" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-6-x-Logrotate/" class="article-date">
  <time datetime="2016-12-14T04:51:19.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-6-x-Logrotate/">CentOS 6.x Logrotate</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>日志文件包含系统内部执行情况的有用信息。在排除故障或服务器性能分析时往往需要检查日志。对于繁忙的服务器，日志文件可能会在短时间内变得非常大。这将成为一个问题，因为服务器会很快耗尽存储空间。此外，操作一个非常大的日志文件往往会非常棘手。<br>logrotate 是可以自动转储、压缩和删除旧日志文件的一个非常有用的工具。例如，你可以设置logrotate将日志文件 /var/log/foo的每30天转储一次，并且删除超过6个月的记录。一旦配置完成，logrotate可以完全自动化操作不需要进行人工交互<br>在主要的Linux发行版中logrotate包通常是默认安装的。如果由于某种原因，logrotate的不存在，您可以使用apt-get或者yum的命令来安装它。<br>Debian或者Ubuntu</p>
<h1 id="apt-get-install-logrotate-cron"><a href="#apt-get-install-logrotate-cron" class="headerlink" title="apt-get install logrotate cron"></a>apt-get install logrotate cron</h1><p>Fedora, CentOS或者RHEL</p>
<h1 id="yum-install-logrotate-crontabs"><a href="#yum-install-logrotate-crontabs" class="headerlink" title="yum install logrotate crontabs"></a>yum install logrotate crontabs</h1><p>logrotate的配置文件是/etc/logrotate.conf，一般不需要修改。可以在/etc/logrotate.d/目录下放入单独的配置文件定义本文档中需要的功能。<br>示例一<br>在第一个例子中，我们将创建一个10 MB的日志文件/var/log/log-file。我们将看到我们如何使用logrotate来管理这个日志文件。<br>我们首先创建一个10M的日志文件，并用随机位来填充它。</p>
<h1 id="touch-var-log-log-file-head-c-10M-lt-dev-urandom-gt-var-log-log-file"><a href="#touch-var-log-log-file-head-c-10M-lt-dev-urandom-gt-var-log-log-file" class="headerlink" title="touch /var/log/log-file# head -c 10M &lt; /dev/urandom &gt; /var/log/log-file"></a>touch /var/log/log-file# head -c 10M &lt; /dev/urandom &gt; /var/log/log-file</h1><p>日志文件已经准备就绪，我们将配置logrotate转储这个日志文件。让我们开始创建转储所需要的logrotate配置文件。</p>
<h1 id="vim-etc-logrotate-d-log-file"><a href="#vim-etc-logrotate-d-log-file" class="headerlink" title="vim /etc/logrotate.d/log-file"></a>vim /etc/logrotate.d/log-file</h1><p>/var/log/log-file {<br>    monthly<br>    rotate<br>5<br>    compress<br>    delaycompress<br>    missingok<br>    notifempty<br>    create<br>644<br>root root<br>    postrotate<br>/usr/bin/killall -<br>HUP rsyslogd<br>    endscript<br>}<br>其中：<br>monthly：每月转储日志文件。其他可能的值是’daily’, ‘weekly’, ‘yearly’。<br>rotate 5：同一时间保存5个归档日志。对于第6个归档，最早的归档文件将被删除。<br>compress：转储完成后，转储的日志将使用gzip压缩。<br>delaycompress：需要与compress选项一起使用，delaycompress参数指示logrotate的不对最新的归档进行压缩。压缩将在下一个转储周期来进行。如果您需要访问的最新备份的存档，这是很有用的。<br>missingok：在转储过程中任何错误都会被忽略，例如，”未找到文件”。<br>notifempty：如果日志文件是空的将不会执行转储。<br>create 644 root root：logrotate归档时可以重命名原始日志文件然后创建一个新的日志文件并赋予与指定的权限。<br>postrotate/endscript：所有其他操作完成后会执行postrotate和endscript间定义的命令。在上面的例子中，处理rsyslogd将重新读取其配置后继续运行。<br>上面的模板是通用的，配置参数可以根据您的要求各不相同。不是所有的参数可能是必要的。<br>示例二<br>在这个例子中，我们想要只在日志文件大小增长超过50MB时转储日志文件。</p>
<h1 id="vim-etc-logrotate-d-log-file-1"><a href="#vim-etc-logrotate-d-log-file-1" class="headerlink" title="vim /etc/logrotate.d/log-file"></a>vim /etc/logrotate.d/log-file</h1><p>/var/log/log-file {    size=50M    rotate 5    create 644<br>root root<br>    postrotate<br>/usr/bin/killall -<br>HUP rsyslogd<br>    endscript<br>}<br>示例三<br>我们希望旧的日志文件根据归档日期进行命名，这可以通过添加dateext参数来实现。</p>
<h1 id="vim-etc-logrotate-d-log-file-2"><a href="#vim-etc-logrotate-d-log-file-2" class="headerlink" title="vim /etc/logrotate.d/log-file"></a>vim /etc/logrotate.d/log-file</h1><p>/var/log/log-file {    size=50M    rotate 5    create 644<br>root root<br>    postrotate<br>/usr/bin/killall -<br>HUP rsyslogd<br>    endscript<br>}</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-6-x-Logrotate/" data-id="ciwps8im2000650cy5gea3j9n" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-6-x-Docker-Swarm-Etcd-Cluster" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-6-x-Docker-Swarm-Etcd-Cluster/" class="article-date">
  <time datetime="2016-12-14T04:50:08.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-6-x-Docker-Swarm-Etcd-Cluster/">CentOS 6.x Docker + Swarm + Etcd Cluster</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://docs.docker.com/engine/installation/linux/centos/" target="_blank" rel="external">https://docs.docker.com/engine/installation/linux/centos/</a></p>
<p>安装Swarm集群<br>环境信息<br>服务器:<br>192.168.99.100 centos-node1<br>192.168.99.101 centos-node2<br>192.168.99.102 centos-node3<br>集群信息:<br>etc服务器: 192.168.99.100:2379<br>swarm manage: 192.168.99.101:3376<br>swarm-node1: 192.168.99.100:2375<br>swarm-node2: 192.168.99.101:2375<br>swarm-node3: 192.168.99.102:2375<br>准备<br>在所有的机器上安装dokcer<br>在centos-node1 上docker pull ystyle/etcd (官方的下载不了,自己做了个一样的)<br>在所有机器上dokcer pull swarm<br>以上三步可以用docker-machine完成<br>开放所有机器的2375端口, centos-node1的2379端口,centos-node2的3376端口<br>安装etcd k-v数据库<br>在centos-node1上执行:</p>
<h1 id="设置当前host的ip"><a href="#设置当前host的ip" class="headerlink" title="设置当前host的ip"></a>设置当前host的ip</h1><p>export HOSTIP=192.168.99.100</p>
<h1 id="启动etcd-k-v服务器"><a href="#启动etcd-k-v服务器" class="headerlink" title="启动etcd k-v服务器"></a>启动etcd <code>k-v</code>服务器</h1><p>docker run -d -v /etc/ssl/certs:/etc/ssl/certs -p 4001:4001 -p 2380:2380 -p 2379:2379<br> –name etcd ystyle/etcd<br> -name etcd0<br> -advertise-client-urls <a href="http://${HOSTIP}:2379,http://${HOSTIP}:4001" target="_blank" rel="external">http://${HOSTIP}:2379,http://${HOSTIP}:4001</a><br> -listen-client-urls <a href="http://0.0.0.0:2379,http://0.0.0.0:4001" target="_blank" rel="external">http://0.0.0.0:2379,http://0.0.0.0:4001</a><br> -initial-advertise-peer-urls <a href="http://${HOSTIP}:2380" target="_blank" rel="external">http://${HOSTIP}:2380</a><br> -listen-peer-urls <a href="http://0.0.0.0:2380" target="_blank" rel="external">http://0.0.0.0:2380</a><br> -initial-cluster-token etcd-cluster-1<br> -initial-cluster etcd0=<a href="http://${HOSTIP}:2380" target="_blank" rel="external">http://${HOSTIP}:2380</a><br> -initial-cluster-state new<br>加入集群<br>在centos-node1执行:<br>docker run -d swarm join –addr=192.168.99.100:2375 etcd://192.168.99.100:2379/swarm<br>在centos-node2执行:<br>docker run -d swarm join –addr=192.168.99.101:2375 etcd://192.168.99.100:2379/swarm<br>在centos-node3执行:<br>docker run -d swarm join –addr=192.168.99.102:2375 etcd://192.168.99.100:2379/swarm<br>启动swarm manage<br>在centos-node2上执行</p>
<h1 id="启动swarm-manage"><a href="#启动swarm-manage" class="headerlink" title="启动swarm manage"></a>启动swarm manage</h1><p>docker run -d -p 3376:3376 -t<br> swarm manage<br>-H 0.0.0.0:3376<br>etcd://192.168.99.100:2379/swarm</p>
<h1 id="检查swarm节点列表"><a href="#检查swarm节点列表" class="headerlink" title="检查swarm节点列表"></a>检查swarm节点列表</h1><p>docker run –rm swarm list etcd://192.168.99.100:2379/swarm</p>
<h1 id="查看swarm集群信息"><a href="#查看swarm集群信息" class="headerlink" title="查看swarm集群信息"></a>查看swarm集群信息</h1><p>export DOCKER_HOST=192.168.99.101:3376<br>docker info</p>
<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>docker run –rm -p 8080:80 nginx:alpine<br>docker ps -a</p>
<h1 id="查看nginx安装到哪台机器上了"><a href="#查看nginx安装到哪台机器上了" class="headerlink" title="查看nginx安装到哪台机器上了"></a>查看nginx安装到哪台机器上了</h1><p>curl -L <a href="http://nginx_host:8080" target="_blank" rel="external">http://nginx_host:8080</a><br>记录<br>如果docker info 出现Error: ID duplicated.删掉/etc/docker/key.json文件(我的虚拟机是直接复制出来的)<br>参考资料<br><a href="https://docs.docker.com/v1.5/swarm/discovery/#using-etcd" target="_blank" rel="external">https://docs.docker.com/v1.5/swarm/discovery/#using-etcd</a><br><a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/" target="_blank" rel="external">https://docs.docker.com/engine/userguide/networking/get-started-overlay/</a><br><a href="https://github.com/docker/swarm" target="_blank" rel="external">https://github.com/docker/swarm</a><br><a href="https://github.com/coreos/etcd" target="_blank" rel="external">https://github.com/coreos/etcd</a><br><a href="https://docs.docker.com/engine/installation/linux/centos/" target="_blank" rel="external">https://docs.docker.com/engine/installation/linux/centos/</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-6-x-Docker-Swarm-Etcd-Cluster/" data-id="ciwps8im0000550cyobwwyg2x" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Ubuntu-14-04-Docker" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/Ubuntu-14-04-Docker/" class="article-date">
  <time datetime="2016-12-14T04:49:11.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/Ubuntu-14-04-Docker/">Ubuntu 14.04 Docker</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Docker 介绍<br>Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。它们不依赖于任何语言、框架或包装系统。<br>三个最重要的基本概念（此处参考来源《Docker从入门到实践》）<br>这三个基本概念是Docker镜像，Docker容器和Docker仓库。<br>【Docker镜像】<br>Docker 镜像就是一个只读的模板。<br>例如：一个镜像可以包含一个完整的 ubuntu 操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。<br>镜像可以用来创建 Docker 容器。<br>Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。<br>【Docker容器】<br>Docker 利用容器来运行应用。<br>容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。<br>可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。<br><em>注：镜像是只读的，容器在启动的时候创建一层可写层作为最上层。<br>【Docker仓库】<br>仓库是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。<br>仓库分为公开仓库（Public）和私有仓库（Private）两种形式。<br>最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。 国内的公开仓库包括 Docker Pool 等，可以提供大陆用户更稳定快速的访问。<br>当然，用户也可以在本地网络内创建一个私有仓库。<br>当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。
</em>注：Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。<br>值得关注的特性：<br>1 文件系统隔离：每个进程容器运行在一个完全独立的根文件系统里。<br>2 资源隔离：系统资源，像CPU和内存等可以分配到不同的容器中，使用cgroup。<br>3 网络隔离：每个进程容器运行在自己的网络空间，虚拟接口和IP地址。<br>4 日志记录：Docker将会收集和记录每个进程容器的标准流（stdout/stderr/stdin），用于实时检索或批量检索。<br>5 变更管理：容器文件系统的变更可以提交到新的映像中，并可重复使用以创建更多的容器。无需使用模板或手动配置。<br>6 交互式shell：Docker可以分配一个虚拟终端并关联到任何容器的标准输入上，例如运行一个一次性交互shell。<br>安装过程<br>我用的系统： UbuntuKylin 14.10 32位系统<br>在ubuntu的官方软件仓库已经包含了docker，可以使用apt-get的方式安装。<br>安装Docker使用apt-get命令:<br>$ apt-get install docker.io<br>启动服务和守护进程<br>$ service docker status<br>$ service docker start<br>几个Docker基本命令<br>首先，让我们通过下面的命令来检查Docker的安装是否正确：<br>docker info<br>如果没有找到此命令，则表示Docker没有正确安装。如果正确安装会输出类似下面的内容：<br>Containers: 0<br>Images: 124<br>Storage Driver: aufs<br> Root Dir: /var/lib/docker/aufs<br> Dirs: 124<br>Execution Driver: native-0.2<br>Kernel Version: 3.16.0-23-generic<br>Operating System: Ubuntu Kylin 14.10<br>到这一步Docker里还没有镜像或是容器。所以，让我们通过命令来拉取一个预建的镜像：<br>sudo docker pull busybox （这里使用的是官方的registry）<br>这里推荐下国内的 docker.cn，速度比官方的快很多<br>使用方法如下：sudo docker pull docker.cn/docker/busybox<br>未完待续。。。。。。<br>参考资料：<br><a href="http://www.linuxidc.com/Linux/2013-10/91050.htm" target="_blank" rel="external">http://www.linuxidc.com/Linux/2013-10/91050.htm</a><br><a href="http://cloud.51cto.com/art/201412/462905.htm" target="_blank" rel="external">http://cloud.51cto.com/art/201412/462905.htm</a><br><a href="http://dockerpool.com/static/books/docker_practice/index.html" target="_blank" rel="external">http://dockerpool.com/static/books/docker_practice/index.html</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/Ubuntu-14-04-Docker/" data-id="ciwps8ing001250cyzoqqy0rf" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-6-x-CAT" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-6-x-CAT/" class="article-date">
  <time datetime="2016-12-14T04:48:22.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-6-x-CAT/">CentOS 6.x CAT</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>CAT(Central Application Tracking)是基于Java开发的实时应用监控平台，为大众点评网提供了全面的监控服务和决策支持。<br>CAT作为大众点评网基础监控组件，它已经在中间件框架（MVC框架，RPC框架，数据库框架，缓存框架等）中得到广泛应用，为点评各业务线提供系统的性能指标、健康状况、基础告警等。<br>地址:<a href="https://github.com/dianping/cat" target="_blank" rel="external">https://github.com/dianping/cat</a><br>require: JAVA6<br>Maven 3<br>Mysql<br>1.安装jdk,maven(操作系统CentOS release 6.5)<br>root@172.16.221.1:~#  yum install jdk1.8.0_40.x86_64<br>root@172.16.221.1:~#wget <a href="http://mirror.olnevhost.net/pub/apache/maven/binaries/apache-maven-3.2.1-bin.tar.gz" target="_blank" rel="external">http://mirror.olnevhost.net/pub/apache/maven/binaries/apache-maven-3.2.1-bin.tar.gz</a><br>root@172.16.221.1:apache-maven# pwd<br>/usr/local/apache-maven<br>root@172.16.221.1:apache-maven# tar xf apache-maven-3.2.1-bin.tar.gz<br>root@172.16.221.1:apache-maven-3.2.1# ll<br>total 40<br>drwxr-xr-x 2 root root  4096 Apr 23 17:20 bin<br>drwxr-xr-x 2 root root  4096 Apr 23 17:20 boot<br>drwxr-xr-x 3 root root  4096 Feb 15  2014 conf<br>drwxr-xr-x 3 root root  4096 Apr 23 17:20 lib<br>-rw-r–r– 1 root root 14865 Feb 15  2014 LICENSE<br>-rw-r–r– 1 root root   182 Feb 15  2014 NOTICE<br>-rw-r–r– 1 root root  2513 Feb 15  2014 README.txt<br>root@172.16.221.1:apache-maven-3.2.1# pwd<br>/usr/local/apache-maven/apache-maven-3.2.1<br>2.环境变量配置<br>root@172.16.221.1:apache-maven-3.2.1# cat /etc/profile<br>…<br>export M2_HOME=/usr/local/apache-maven/apache-maven-3.2.1<br>export M2=$M2_HOME/bin<br>export PATH=$M2:$PATH</p>
<p>root@172.16.221.1:apache-maven-3.2.1# source /etc/profile<br>3 测试<br>root@172.16.221.1:~# java -version<br>java version “1.8.0_40”<br>Java(TM) SE Runtime Environment (build 1.8.0_40-b26)<br>Java HotSpot(TM) 64-Bit Server VM (build 25.40-b25, mixed mode)</p>
<p>root@172.16.221.1:~# mvn -v<br>Apache Maven 3.2.1 (ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9; 2014-02-15T01:37:52+08:00)<br>Maven home: /usr/local/apache-maven/apache-maven-3.2.1<br>Java version: 1.8.0_40, vendor: Oracle Corporation<br>Java home: /usr/java/jdk1.8.0_40/jre Default locale: en_US, platform encoding: UTF-8<br>OS name: “linux”, version: “2.6.32-431.el6.x86_64”, arch: “amd64”, family: “unix”<br>4 Clone CAT<br>root@172.16.221.1:opt# git clone git@github.com:dianping/cat.git<br>5 在CAT目录下,用maven构建项目<br>root@172.16.221.1:cat# pwd<br>/opt/cat<br>root@172.16.221.1:cat# mvn clean install -DskipTests<br>6 配置CAT的环境<br>root@172.16.221.1:cat# mav cat:install<br>注意 需要对/data/appdatas/cat和/data/applogs/cat有读写权限<br>7 运行CAT<br>root@172.16.221.1:cat-home# pwd<br>/opt/cat/cat-home<br>root@172.16.221.1:cat-home#nohup mvn jetty:run &amp;<br>8 打开浏览器，输入<a href="http://172.16.221.1:2281/cat/。" target="_blank" rel="external">http://172.16.221.1:2281/cat/。</a></p>
<p>或者在cat目录下输入 mvn eclipse:clean eclipse:eclipse 然后将项目导入到eclipse中，运行cat-home项目里得‘com.dianping.cat.TestServer’来启动CAT.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-6-x-CAT/" data-id="ciwps8ilw000350cym52h3u5r" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-7-x-Ceph" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-7-x-Ceph/" class="article-date">
  <time datetime="2016-12-14T04:46:08.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-7-x-Ceph/">CentOS 7.x Ceph</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://docs.ceph.org.cn/" target="_blank" rel="external">http://docs.ceph.org.cn/</a></p>
<p>[root@node2 ~]# rpm -ivh <a href="http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm" target="_blank" rel="external">http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpm</a></p>
<p>安装：</p>
<p>yum install scsi-target-utils -y</p>
<p>启动服务<br>[root@node4 ~]# systemctl enable tgtd.service<br>Created symlink from /etc/systemd/system/multi-user.target.wants/tgtd.service to /usr/lib/systemd/system/tgtd.service.<br>[root@node4 ~]# systemctl start tgtd.service</p>
<p>确认一下有没有端口起来<br>[root@node4 ~]# netstat -anlpt | grep 3260<br>tcp        0      0 0.0.0.0:3260            0.0.0.0:<em>               LISTEN      8285/tgtd<br>tcp6       0      0 :::3260                 :::</em>                    LISTEN      8285/tgtd<br>[root@node4 ~]#</p>
<p>设为开机自启动：<br>[root@node4 ~]# chkconfig tgtd on<br>Note: Forwarding request to ‘systemctl enable tgtd.service’.<br>[root@node4 ~]# </p>
<p>dd if=/dev/zero of=/mycephfs/cephimg2 bs=1024M count=1000</p>
<p>vim /etc/tgt/targets.conf</p>
<target iqn.2016-09.node2:cephimg2=""><br>  backing-store /mycephfs/cephimg2<br></target>

<p>systemctl restart tgtd.service</p>
<p>tgtadm –lld iscsi –op new –mode logicalunit –tid 1 –lun 3 -b /mycephfs/cephimg2</p>
<p>tgtadm –lld iscsi –op show –mode target</p>
<p>dd if=/dev/zero of=/mycephfs/cephimg1 bs=1024M count=4000</p>
<p>vim /etc/tgt/targets.conf</p>
<target iqn.2016-10.node2:cephimg3=""><br>  backing-store /mycephfs/cephimg3<br></target>

<p>systemctl restart tgtd.service</p>
<p>tgtadm –lld iscsi –op new –mode logicalunit –tid 1 –lun 3 -b /mycephfs/cephimg1</p>
<p>tgtadm –lld iscsi –op show –mode target</p>
<p>10.6.100.39 node4# dd if=/dev/zero of=/data/img/img1  bs=1024M count=1024</p>
<p>10.6.100.39 node4# dd if=/dev/zero of=/data/img/img2  bs=1024M count=1024<br>vim /etc/tgt/targets.conf</p>
<target iqn.2016-08.n5:img2=""><br>  backing-store /data/img/img2<br></target>

<p>systemctl restart tgtd.service</p>
<p>查看信息<br>tgtadm –lld iscsi –op show –mode target</p>
<p>在安装ceph之前推荐把所有的ceph节点设置成无需密码ssh互访，配置hosts支持主机名互访，同步好时间，并关闭iptables和selinux。<br>vim /etc/selinux/config</p>
<p>systemctl stop firewalld.service<br>systemctl disable firewalld.service</p>
<p>yum install ntp -y<br>systemctl enable ntpd<br>systemctl status ntpd<br>ntpdate -u 202.108.6.95</p>
<p>node1<br>vim /etc/ntp.conf<br>server 202.108.6.95 prefer<br>service ntpd start<br>ntpstat</p>
<p>node2、3<br>vim /etc/ntp.conf<br>server node1<br>ntpdate -u node1<br>service ntpd start<br>ntpstat</p>
<p>vim /etc/hosts<br>10.6.0.166  node1<br>10.6.0.167  node2<br>10.6.0.168  node3<br>10.6.0.169  node4</p>
<p>vim /etc/hosts<br>10.6.0.171  node1<br>10.6.0.172  node2<br>10.6.0.173  node3<br>10.6.0.174  node4</p>
<p>vim /etc/hosts<br>10.6.4.166  node1<br>10.6.4.167  node2<br>10.6.4.168  node3<br>10.6.4.169  node4<br>10.6.4.170  node5<br>10.6.4.171  node6<br>10.6.4.172  node7<br>10.6.4.173  node8<br>10.6.4.174  node9</p>
<p>node1<br>ssh-keygen -t rsa<br>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>chmod 600 ~/.ssh/authorized_keys</p>
<p>[root@node1 ~]# cat ~/.ssh/authorized_keys<br>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCxp6jN9BsZf6V+pQEksQQaOdesJ8jOPAkudEpsEnNFdUW2PzK/Hrn+s3TrdgekTAWRvjXELQ/S45Z/eK+r2C14rzLaLN1Twq6fJfjF2B+tGvejG/gWzDCx/FPLTv7155UwNCObDCPveV3Ey/bkL681CN8YezBh58IX0uMk5jOJAjlWexVOB7oE1iAmP8dB/B2mbk6arX7QjTr79Fp5WzVM50BNsDcTG9rccKrKs7XPqYPTCfMB/b0gfeoOnQ4B4bY/PL0AdluPQSpD0d2/I2jefDcL89CBeH16oFtTGFFsKnDh4WgcDwlEsS7KYvV8N/mqu8Ym1VAep/7b83KoUyOH root@node1<br>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC+kjsX4sSLByfEQYnLvyvHDBUtup4MVG7EPkl7MyTabRapn0O1LA2ZvybIBjSSmiuRAEy9CF3tbkg+2wFjvfwa1M8x/83NAQ1AOZhWHjTzpdFAGiIetEiunaDVDLqb9c1q+U3ETFRFxPaaoBLijGNFVWFovJ8sT25i76gluDQ+yIT9vW6gbnjyy2Q32HYhsxKZj4PnG1YbdDa3H0FBm2b7kVZ7SpZq2xbsmZqF+gY9RfqWTjcHHOplyrZeCh6HHPPyr9XvZ/DpcHXLGwOx7PPkhnn5Q8Byrofb5dJfxbcfg22aQsdnvD59WxzK4Gfqw5muGga31fEVJKLzn74+MWa9 root@node2<br>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDKOsjb53KEQ88zJXGx2B3rnn0/Mnz0ObNb/Ees13+OgWyAjnaQWNiRVuqLTcQU5DTjGfQPy1oYq6TBOdgXv10BShU0LXv2Ju2YBcsV50PVGZtVhQ6iX/+qDhs0vv0lLt3xPQzpbxQRV5FXMb5HpPu2KYtVtaRao9CdGhteLrjQf2MymCNrS8BfjeFEPNEaD0gyLzTsPbN9IlvHYToAyLrqlQfp9V6YqgarRHtafDcq/K6H7w3rynHDOR2wpifscbTezWLNOpSDMthMqvmhFX9nXOGChTnc0k4+1Fj4V2sto7/vKX1gdN2Hha1APT9u9XXDBC0ur5HQqnpiSWAYqlLr root@node3<br>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0WQ5Vu0HWmWQslihi4Uv8MjvwUGanw/exy3tOU8rUxnB0DfWR5e98GpEE37NCU6Y8Q+Ppy3H3RxUSdfw/cFhaz03bDjRphdHPU/aTFWJPBXo6uJjV9s34WaP2q6xKd2/NsgTm+FIiD0O37UtqVUGvvPtqU1QiaZOOxSlmLtjBGLM8WmEjjCpHS+AJ3ceWR16lkry5+F1DnrEITdEmVTy/zCPuQk9Ey32jU39AELm1gtYHcaXvE1Ir8+dZ83fmAleFnT7sONARd4rHjYAyuRx47kjOea9Y89KIbF1SUTgW7nystZ1ULPIeotGeuGgsl2X7bRKUJxzE76nybZruo7Hp root@node4<br>[root@node1 ~]#</p>
<p>scp ~/.ssh/authorized_keys root@node2:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node3:~/.ssh/</p>
<p>node2<br>ssh-keygen -t rsa<br>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>chmod 600 ~/.ssh/authorized_keys</p>
<p>scp ~/.ssh/authorized_keys root@node1:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node3:~/.ssh/</p>
<p>node3<br>ssh-keygen -t rsa<br>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br>chmod 600 ~/.ssh/authorized_keys</p>
<p>scp ~/.ssh/authorized_keys root@node1:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node2:~/.ssh/</p>
<p>scp ~/.ssh/authorized_keys root@node4:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node5:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node6:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node7:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node8:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node9:~/.ssh/<br>scp ~/.ssh/authorized_keys root@node1:~/.ssh/</p>
<p>１、实验环境说明：<br>当前实验环境使用了4台主机node1~node4，node1为管理节点。<br>2、部署工具：<br>　　Ceph官方推出了一个用python写的工具 cpeh-deploy</p>
<p>添加ceph源</p>
<p>vim /etc/yum.repos.d/ceph.repo<br>[ceph]<br>name=ceph<br>baseurl=<a href="http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/" target="_blank" rel="external">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/</a><br>gpgcheck=0<br>[ceph-noarch]<br>name=cephnoarch<br>baseurl=<a href="http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/" target="_blank" rel="external">http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/</a><br>gpgcheck=0</p>
<p>yum makecache</p>
<p>3、安装步骤<br>所有节点安装软件<br>yum install ceph ceph-deploy -y</p>
<p>创建工作目录，用于存放生成的配置文件和秘钥等信息<br>mkdir /ceph ; cd /ceph<br>到管理主机上的/ceph目录操作,创建一个新集群，并设置node1为mon节点<br>ceph-deploy new node1<br>ceph-deploy new node4<br>执行完毕后，可以看到/ceph目录中生成了三个文件，其中有一个配置文件可以做各种参数优化，据说ceph的优化参数接近1000项。（注意，在osd进程生成并挂载使用后，想修改配置需要使用命令行工具，修改配置文件是无效的，所以需要提前规划好优化的参数。）<br>在ceph.conf中添加四个最基本的设置<br>echo “osd pool default size = 3” &gt;&gt; ceph.conf<br>echo “osd_pool_default_min_size = 2” &gt;&gt; ceph.conf<br>cat ceph.conf</p>
<p>激活监控节点<br>ceph-deploy mon create-initial</p>
<p>接下来创建osd节点<br>node1<br>mkdir /data/osd01<br>chmod -R 777 /data/osd01</p>
<p>node2<br>mkdir /data/osd02<br>chmod -R 777 /data/osd02</p>
<p>mkdir /var/local/osd2<br>chmod -R 777 /var/local/osd2</p>
<p>node3<br>mkdir /data/osd03<br>chmod -R 777 /data/osd03</p>
<p>mkdir /var/local/osd3<br>chmod -R 777 /var/local/osd3</p>
<p>node4<br>mkdir /var/local/osd4<br>chmod -R 777 /var/local/osd4</p>
<p>ceph-deploy osd prepare node1:/data/osd01 node2:/data/osd02 node3:/data/osd03</p>
<p>ceph-deploy osd activate node1:/data/osd01 node2:/data/osd02 node3:/data/osd03</p>
<p>ceph-deploy osd prepare node2:/var/local/osd2 node3:/var/local/osd3 node4:/var/local/osd4</p>
<p>ceph-deploy osd activate node2:/var/local/osd2 node3:/var/local/osd3 node4:/var/local/osd4</p>
<p>将管理节点上的配置文件同步到其他节点上<br>ceph-deploy admin node1 node2 node3</p>
<p>所有节点<br>chmod +r /etc/ceph/ceph.client.admin.keyring</p>
<p>ceph health<br>HEALTH_OK</p>
<p>ceph -s<br>[root@node1 ceph]# ceph -s<br>    cluster 8828b04f-e1f9-49fa-be83-52948e1ed407<br>     health HEALTH_OK<br>     monmap e1: 1 mons at {node1=10.6.4.166:6789/0}<br>            election epoch 3, quorum 0 node1<br>     osdmap e14: 3 osds: 3 up, 3 in<br>            flags sortbitwise<br>      pgmap v26: 64 pgs, 1 pools, 0 bytes data, 0 objects<br>            30468 MB used, 9341 MB / 39810 MB avail<br>                  64 active+clean<br>[root@node1 ceph]#</p>
<p>添加元数据服务器</p>
<p>至少需要一个元数据服务器才能使用 CephFS ，执行下列命令创建元数据服务器：</p>
<p>ceph-deploy mds create node1<br>ceph-deploy mds create node4<br>Note 当前生产环境下的 Ceph 只能运行一个元数据服务器。你可以配置多个，但现在我们还不会为多个元数据服务器的集群提供商业支持。<br>[root@node1 ceph]# ceph-deploy mds create node1<br>[root@node1 ceph]# ceph osd pool create test1 64<br>pool ‘test1’ created<br>[root@node1 ceph]# ceph osd pool create test2 64<br>pool ‘test2’ created<br>[root@node1 ceph]# ceph fs new cephfs test2 test1<br>new fs with metadata pool 2 and data pool 1<br>[root@node1 ceph]# ceph -s<br>    cluster 8828b04f-e1f9-49fa-be83-52948e1ed407<br>     health HEALTH_OK<br>     monmap e1: 1 mons at {node1=10.6.4.166:6789/0}<br>            election epoch 3, quorum 0 node1<br>      fsmap e5: 1/1/1 up {0=node1=up:active}<br>     osdmap e19: 3 osds: 3 up, 3 in<br>            flags sortbitwise<br>      pgmap v52: 192 pgs, 3 pools, 2068 bytes data, 20 objects<br>            30473 MB used, 9336 MB / 39810 MB avail<br>                 192 active+clean<br>[root@node1 ceph]#</p>
<p>[root@node1 ceph]# ceph osd tree<br>ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY<br>-1 0.03809 root default<br>-2 0.01270     host node2<br> 0 0.01270         osd.0       up  1.00000          1.00000<br>-3 0.01270     host node3<br> 1 0.01270         osd.1       up  1.00000          1.00000<br>-4 0.01270     host node4<br> 2 0.01270         osd.2       up  1.00000          1.00000<br>[root@node1 ceph]#  </p>
<p>Ceph 存储集群需要至少一个 Monitor 才能运行。为达到高可用，典型的 Ceph 存储集群会运行多个 Monitors，这样在单个 Monitor 失败时不会影响 Ceph 存储集群的可用性。Ceph 使用 PASOX 算法，此算法要求有多半 monitors（即 1 、 2:3 、 3:4 、 3:5 、 4:6 等 ）形成法定人数。</p>
<p>新增两个监视器到 Ceph 集群。</p>
<p>ceph-deploy mon add node2 node3<br>新增 Monitor 后，Ceph 会自动开始同步并形成法定人数。你可以用下面的命令检查法定人数状态：</p>
<p>ceph quorum_status –format json-pretty<br>Tip 当你的 Ceph 集群运行着多个 monitor 时，各 monitor 主机上都应该配置 NTP ，而且要确保这些 monitor 位于 NTP 服务的同一级。</p>
<p>CephFS的使用<br>在客户端上操作：<br>1、安装客户端挂载软件<br>yum install ceph-fuse -y<br>2、创建挂载目录<br>mkdir /mycephfs<br>3、证书来源位置<br>[root@node4 ~]# more /etc/ceph/ceph.client.admin.keyring<br>[client.admin]<br>        key = AQBqN8ZX5wC7NRAAnl8I0VBqoFPMJawT6x3hDw==<br>[root@node4 ~]#<br>4、模拟挂载<br>[root@node4 ~]# mount -t ceph node1:6789:/ /mycephfs -v -o name=admin,secret=AQBqN8ZX5wC7NRAAnl8I0VBqoFPMJawT6x3hDw==<br>parsing options: rw,name=admin,secret=AQBqN8ZX5wC7NRAAnl8I0VBqoFPMJawT6x3hDw==</p>
<p>[root@node2 ~]# more /etc/ceph/ceph.client.admin.keyring<br>[client.admin]<br>        key = AQBOj9dXVED0AhAAAmP5gKxGA1/562ERJF068w==<br>[root@node2 ~]#<br>mount -t ceph node4:6789:/ /mycephfs -v -o name=admin,secret=AQBOj9dXVED0AhAAAmP5gKxGA1/562ERJF068w==</p>
<p>[root@node4 ~]# df -h<br>Filesystem               Size  Used Avail Use% Mounted on<br>/dev/mapper/centos-root   13G   10G  3.1G  77% /<br>devtmpfs                 479M     0  479M   0% /dev<br>tmpfs                    494M   84K  494M   1% /dev/shm<br>tmpfs                    494M  6.9M  488M   2% /run<br>tmpfs                    494M     0  494M   0% /sys/fs/cgroup<br>/dev/xvda1               497M  140M  358M  29% /boot<br>tmpfs                     99M   16K   99M   1% /run/user/42<br>tmpfs                     99M     0   99M   0% /run/user/0<br>10.6.4.166:6789:/         39G   30G  9.2G  77% /mycephfs<br>[root@node4 ~]#<br>5、另外一种命令挂载方式<br>[root@ceph-client ~]# mount  -t ceph node1:6789:/ /mycephfs -v -oname=admin,secretfile=/etc/ceph/ceph.client.admin.keyring<br>6、若果有多个mon监控节点，可以挂载多可节点，保证了cephFS的安全行，当有一个节点down的时候不影响写入数据<br>[root@client ~]# mount.cephnode01,node02,node03:/ /mycephfs -v -o name=admin,secret= AQCnrMZUaFsiCxAAXzM3aF9WjUBnwbN6PtvZEw==<br>7、验证挂载信息：<br>df -h<br>Filesystem          Size Used Avail Use% Mounted on<br>/dev/sda5           1.7T 4.8G  1.7T   1% /data<br>/dev/rbd0            78G   56M  78G   1% /rbdtest<br>172.16.2.27:6789:/  8.2T  9.8G 8.2T   1% /mycephfs<br>8、把挂载的信息写到fstab里<br>[root@client ~]# vi /etc/fstab<br>172.16.2.27,172.16.2.28,172.16.2.29:/  /mycephfs  ceph  name=admin,secret= AQCnrMZUaFsiCxAAXzM3aF9WjUBnwbN6PtvZEw==,noatime    0<br>9、指定key文件的调用方式<br>sudoceph-fuse -k ./ceph.client.admin.keyring -m 192.168.40.107:6789 ~/mycephfs<br>10、挂载后查看结果，注意观察类型<br>df-Th<br>11、取消挂载<br>取消挂载的操作如下:<br>umount /mnt/mycephfs</p>
<p>sudo mkdir /mnt/mycephfs<br>sudo mount -t ceph {ip-address-of-monitor}:6789:/ /mnt/mycephfs<br>或者<br>sudo mkdir /home/{username}/cephfs<br>sudo ceph-fuse -m {ip-address-of-monitor}:6789 /home/{username}/cephfs</p>
<h1 id="df-–h-查看"><a href="#df-–h-查看" class="headerlink" title="df –h   查看"></a>df –h   查看</h1><p>[root@ceph-client ~]# mkdir /mnt/mycephfs<br>[root@ceph-client ~]# mount  -t ceph 10.240.240.211:6789:/ /mnt/mycephfs -v -o name=admin,secret=AQDT9pNTSFD6NRAAoZkAgx21uGQ+DM/k0rzxow==<br>10.240.240.211:6789:/ on /mnt/mycephfs type ceph (rw,name=admin,secret=AQDT9pNTSFD6NRAAoZkAgx21uGQ+DM/k0rzxow==) </p>
<p>#上述命令中的name和secret参数值来自monitor的/etc/ceph/keyring文件：<br>[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring<br>[client.admin]<br>        key = AQDT9pNTSFD6NRAAoZkAgx21uGQ+DM/k0rzxow==</p>
<p>检查ceph集群状态常用命令<br>ceph health   //ceph健康状态<br>ceph status   //ceph当前全部状态<br>ceph -w //实时监控ceph状态及变化<br>ceph osddump                                //所有osd详细状态<br>ceph osd tree        //osd所在位置，及状态<br>cephquorum_status    //mon优先级状态<br>ceph mon dump    //mon节点状态<br>ceph mds dump    //mds详细状态</p>
<p>添加 OSD</p>
<p>你运行的这个三节点集群只是用于演示的，把 OSD 添加到 monitor 节点就行。</p>
<p>yum install ceph ceph-deploy -y</p>
<p>node5<br>mkdir /var/local/osd5<br>chmod -R 777 /var/local/osd5</p>
<p>然后，从 ceph-deploy 节点准备 OSD 。</p>
<p>ceph-deploy osd prepare node5:/var/local/osd5<br>最后，激活 OSD 。</p>
<p>ceph-deploy osd activate node5:/var/local/osd5</p>
<p>一旦你新加了 OSD ， Ceph 集群就开始重均衡，把归置组迁移到新 OSD 。可以用下面的 ceph 命令观察此过程：</p>
<p>ceph -w</p>
<p>你应该能看到归置组状态从 active + clean 变为 active ，还有一些降级的对象；迁移完成后又会回到 active + clean 状态（ Control-C 退出）。</p>
<p>ceph-deploy admin node4</p>
<p>chmod +r /etc/ceph/ceph.client.admin.keyring</p>
<p><a href="http://docs.ceph.org.cn/rados/operations/add-or-rm-osds/" target="_blank" rel="external">http://docs.ceph.org.cn/rados/operations/add-or-rm-osds/</a><br><a href="http://docs.ceph.com/docs/master/rados/operations/operating/" target="_blank" rel="external">http://docs.ceph.com/docs/master/rados/operations/operating/</a></p>
<p>移除OSD<br>ceph osd out 0<br>[root@node1 ceph]# systemctl stop ceph-osd@0<br>[root@node1 ceph]# ceph osd rm 0<br>removed osd.0<br>[root@node1 ceph]# ceph osd tree<br>ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY<br>-1 0.05078 root default<br>-2 0.01270     host node1<br> 0 0.01270         osd.0      DNE        0<br>-3 0.01270     host node2<br> 1 0.01270         osd.1       up  1.00000          1.00000<br>-4 0.01270     host node3<br> 2 0.01270         osd.2       up  1.00000          1.00000<br>-5 0.01270     host node4<br> 3 0.01270         osd.3       up  1.00000          1.00000<br>[root@node1 ceph]#</p>
<p>cephfs的删除<br>remove所有节点上的ceph<br>ceph-deploy purge node{1..3}</p>
<p>清除所有数据<br>ceph-deploy purgedata node{1..3}</p>
<p>清除所有秘钥文件<br>ceph-deploy forgetkeys</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-7-x-Ceph/" data-id="ciwps8imc000c50cyyl833nfp" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-CentOS-7-x-Docker" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/14/CentOS-7-x-Docker/" class="article-date">
  <time datetime="2016-12-14T04:44:44.000Z" itemprop="datePublished">2016-12-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/14/CentOS-7-x-Docker/">CentOS 7.x Docker</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://www.gitbook.com/book/yeasy/docker_practice/details" target="_blank" rel="external">https://www.gitbook.com/book/yeasy/docker_practice/details</a><br><a href="https://hub.docker.com/explore/" target="_blank" rel="external">https://hub.docker.com/explore/</a><br><a href="https://github.com/shipyard/shipyard" target="_blank" rel="external">https://github.com/shipyard/shipyard</a><br><a href="https://docs.docker.com/engine/installation/linux/" target="_blank" rel="external">https://docs.docker.com/engine/installation/linux/</a></p>
<p>[root@node4 ~]# curl -sSL <a href="http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet" target="_blank" rel="external">http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet</a> | sh -</p>
<p>[root@node4 ~]# service docker start<br>Redirecting to /bin/systemctl start  docker.service<br>[root@node4 ~]#<br>[root@node4 ~]# docker run hello-world</p>
<p> [root@node4 ~]# chkconfig docker on<br>Note: Forwarding request to ‘systemctl enable docker.service’.<br>Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.<br>[root@node4 ~]#<br>docker run -i -t ubuntu:12.04 /bin/bash</p>
<p>docker pull hub.c.163.com/public/centos:6.5<br>Digest: sha256:92fd18cda0cbf63bb4b5ffd0c073a9d689d5b202e3b491cbc74488fff1c37d11<br>Status: Downloaded newer image for hub.c.163.com/public/centos:6.5<br>docker run -i -t hub.c.163.com/public/centos:6.5 /bin/bash</p>
<p>docker pull index.tenxcloud.com/tenxcloud/nodejs<br>Digest: sha256:7b214df4b3ef751f7a2852e14b7d9ed809718251770cf97b57929e61a6eb6458<br>Status: Downloaded newer image for index.tenxcloud.com/tenxcloud/nodejs:latest</p>
<p>[root@node4 ~]# docker images<br>REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE<br>ubuntu              12.04               a11493a01736        2 weeks ago         103.6 MB<br>hello-world         latest              c54a2cc56cbb        10 weeks ago        1.848 kB<br>[root@node4 ~]#</p>
<p>[root@node4 ~]# docker ps –all<br>CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                      PORTS               NAMES<br>ae816f2e5d0e        ubuntu:12.04        “/bin/bash”         55 minutes ago      Exited (0) 54 minutes ago                       elated_bardeen<br>f16fbeb5fda3        hello-world         “/hello”            56 minutes ago      Exited (0) 56 minutes ago                       trusting_mclean<br>97b80da7cc38        hello-world         “/hello”            56 minutes ago      Exited (0) 56 minutes ago                       stupefied_noether<br>[root@node4 ~]#</p>
<p>[root@node4 ~]# docker rm 97b80da7cc38 </p>
<p>docker logs elated_bardeen<br>docker stop elated_bardeen<br>docker restart elated_bardeen<br>docker start elated_bardeen</p>
<p>保存镜像<br>docker save -o ubuntu_12.04.tar ubuntu:12.04<br>docker save -o node_latest.tar node:latest<br>载入镜像<br>可以使用 docker load 从导出的本地文件中再导入到本地镜像库，例如<br>docker load –input ubuntu_12.04.tar<br>docker load &lt; ubuntu_12.04.tar<br>这将导入镜像以及其相关的元数据信息（包括标签等）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/14/CentOS-7-x-Docker/" data-id="ciwps8ime000e50cynlew75er" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Python-自定义函数的特殊属性" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/13/Python-自定义函数的特殊属性/" class="article-date">
  <time datetime="2016-12-13T09:26:53.000Z" itemprop="datePublished">2016-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/13/Python-自定义函数的特殊属性/">Python 自定义函数的特殊属性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Python 中通过函数定义所创建的用户自定义函数对象均具有一些特殊属性，需要注意的是这里介绍的是自定义函数（function类型）的特殊属性，而非方法（method 类型）的特殊属性，函数和方法的特熟属性以及默认的返回值可能不尽相同。</p>
<p>对于大多数特殊属性，可以通过下面这个例子示范一下：</p>
<p>class Test():</p>
<pre><code>def func(self, v = &apos;dog&apos;):
    &apos;&apos;&apos;这里演示一个闭包函数&apos;&apos;&apos;

    name = &apos;dobi&apos;
    def inn_func(age = 1):
        print(name, v, age)

    return inn_func
</code></pre><p>test = Test()<br>clsfunc = test.func()</p>
<p>首先看一下方法与函数的区别：实例的函数为bound method，而类的函数以及闭包均为function，需要强调的是 Python 2.x 中类的函数为unbound method，这点与Python 3.x 有所不同，本文则基于 Python 3.51 整理。</p>
<p>print(Test.func)</p>
<h1 id=""><a href="#" class="headerlink" title=""></a><function test.func="" at="" 0x0000020f0766e268=""></function></h1><p>print(test.func)</p>
<h1 id="-1"><a href="#-1" class="headerlink" title=""></a><bound method="" test.func="" of="" <__main__.test="" object="" at="" 0x0000020f077e5da0="">&gt;</bound></h1><p>print(clsfunc)</p>
<h1 id="-2"><a href="#-2" class="headerlink" title=""></a><function test.func.<locals="">.inn_func at 0x0000020F071D7F28&gt;</function></h1><p><strong>doc</strong></p>
<p>可写；用于获取函数的文档说明，如果没有，则返回 None。</p>
<p>print(‘Test.func.<strong>doc</strong>:’, Test.func.<strong>doc</strong>)</p>
<h1 id="Test-func-doc-这里演示一个闭包函数"><a href="#Test-func-doc-这里演示一个闭包函数" class="headerlink" title="Test.func.doc: 这里演示一个闭包函数"></a>Test.func.<strong>doc</strong>: 这里演示一个闭包函数</h1><p>Test.func.<strong>doc</strong> = ‘ddd’  #注意，这里是 Test,不能是 test<br>print(‘Test.func.<strong>doc</strong>:’, Test.func.<strong>doc</strong>)</p>
<h1 id="Test-func-doc-ddd"><a href="#Test-func-doc-ddd" class="headerlink" title="Test.func.doc: ddd"></a>Test.func.<strong>doc</strong>: ddd</h1><p><strong>name</strong></p>
<p>可写；获取函数的名称。</p>
<p>print(‘Test.func.<strong>name</strong>:’, Test.func.<strong>name</strong>)</p>
<h1 id="Test-func-name-func"><a href="#Test-func-name-func" class="headerlink" title="Test.func.name: func"></a>Test.func.<strong>name</strong>: func</h1><p>Test.func.<strong>name</strong> = ‘pet’<br>print(‘Test.func.<strong>name</strong>:’, Test.func.<strong>name</strong>)</p>
<h1 id="Test-func-name-pet"><a href="#Test-func-name-pet" class="headerlink" title="Test.func.name: pet"></a>Test.func.<strong>name</strong>: pet</h1><p><strong>qualname</strong></p>
<p>可写；获取函数的qualname：点示法显示函数名称、所在的类、模块等梯级地址。</p>
<p>print(‘Test.func.<strong>qualname</strong>:’, Test.func.<strong>qualname</strong>)</p>
<h1 id="Test-func-qualname-Test-func"><a href="#Test-func-qualname-Test-func" class="headerlink" title="Test.func.qualname: Test.func"></a>Test.func.<strong>qualname</strong>: Test.func</h1><p>Test.func.<strong>qualname</strong> = ‘path’<br>print(‘Test.func.<strong>qualname</strong>:’, Test.func.<strong>qualname</strong>)</p>
<h1 id="Test-func-qualname-path"><a href="#Test-func-qualname-path" class="headerlink" title="Test.func.qualname: path"></a>Test.func.<strong>qualname</strong>: path</h1><p><strong>module</strong></p>
<p>可写；返回函数所在的模块，如果无则返回None。</p>
<p>print(‘Test.func.<strong>module</strong>:’, Test.func.<strong>module</strong>)</p>
<h1 id="Test-func-module-main"><a href="#Test-func-module-main" class="headerlink" title="Test.func.module: main"></a>Test.func.<strong>module</strong>: <strong>main</strong></h1><p>Test.func.<strong>module</strong> = ‘a’<br>print(‘Test.func.<strong>module</strong>:’, Test.func.<strong>module</strong>)</p>
<h1 id="Test-func-module-a"><a href="#Test-func-module-a" class="headerlink" title="Test.func.module: a"></a>Test.func.<strong>module</strong>: a</h1><p><strong>defaults</strong></p>
<p>可写；以元组的形式返回函数的默认参数，如果无默认参数则返回None。</p>
<p>print(‘Test.func.<strong>defaults</strong>:’, Test.func.<strong>defaults</strong>)</p>
<h1 id="Test-func-defaults-‘dog’"><a href="#Test-func-defaults-‘dog’" class="headerlink" title="Test.func.defaults: (‘dog’,)"></a>Test.func.<strong>defaults</strong>: (‘dog’,)</h1><p>Test.func.<strong>defaults</strong> = (‘cat’,)<br>print(‘Test.func.<strong>defaults</strong>:’, Test.func.<strong>defaults</strong>)</p>
<h1 id="Test-func-defaults-‘cat’"><a href="#Test-func-defaults-‘cat’" class="headerlink" title="Test.func.defaults: (‘cat’,)"></a>Test.func.<strong>defaults</strong>: (‘cat’,)</h1><p>print(‘clsfunc.<strong>defaults</strong>:’, clsfunc.<strong>defaults</strong>)</p>
<h1 id="clsfunc-defaults-1"><a href="#clsfunc-defaults-1" class="headerlink" title="clsfunc.defaults: (1,)"></a>clsfunc.<strong>defaults</strong>: (1,)</h1><p>clsfunc.<strong>defaults</strong> = (2,)<br>print(‘clsfunc.<strong>defaults</strong>:’, clsfunc.<strong>defaults</strong>)</p>
<h1 id="clsfunc-defaults-2"><a href="#clsfunc-defaults-2" class="headerlink" title="clsfunc.defaults: (2,)"></a>clsfunc.<strong>defaults</strong>: (2,)</h1><p><strong>code</strong></p>
<p>可写；返回已编译的函数对象。</p>
<p>print(‘Test.func.<strong>code</strong>:’, Test.func.<strong>code</strong>)</p>
<h1 id="Test-func-code"><a href="#Test-func-code" class="headerlink" title="Test.func.code:"></a>Test.func.<strong>code</strong>:</h1><p>def func2():print(‘cat’)<br>Test.func.<strong>code</strong> = func2.<strong>code</strong><br>Test.func()</p>
<h1 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h1><p>print(‘Test.func.<strong>code</strong>:’, Test.func.<strong>code</strong>)</p>
<h1 id="Test-func-code-1"><a href="#Test-func-code-1" class="headerlink" title="Test.func.code:"></a>Test.func.<strong>code</strong>:</h1><p><strong>globals</strong></p>
<p>只读，以字典的形式返回函数所在的全局命名空间所定义的全局变量。</p>
<p>print(‘Test.func.<strong>globals</strong>:’, Test.func.<strong>globals</strong>)</p>
<h1 id="Test-func-globals"><a href="#Test-func-globals" class="headerlink" title="Test.func.globals: {"></a>Test.func.<strong>globals</strong>: {</h1><h1 id="‘cached‘-None"><a href="#‘cached‘-None" class="headerlink" title="‘cached‘: None,"></a>‘<strong>cached</strong>‘: None,</h1><h1 id="‘Test’"><a href="#‘Test’" class="headerlink" title="‘Test’: ,"></a>‘Test’: <class '__main__.test'="">,</class></h1><h1 id="‘builtins‘"><a href="#‘builtins‘" class="headerlink" title="‘builtins‘: ,"></a>‘<strong>builtins</strong>‘: <module 'builtins'="" (built-in)="">,</module></h1><h1 id="‘func2’"><a href="#‘func2’" class="headerlink" title="‘func2’: ,"></a>‘func2’: <function func2="" at="" 0x0000020f077d3c80="">,</function></h1><h1 id="‘spec‘-None"><a href="#‘spec‘-None" class="headerlink" title="‘spec‘: None,"></a>‘<strong>spec</strong>‘: None,</h1><h1 id="‘doc‘-None"><a href="#‘doc‘-None" class="headerlink" title="‘doc‘: None,"></a>‘<strong>doc</strong>‘: None,</h1><h1 id="‘file‘-‘D-…-a-py’"><a href="#‘file‘-‘D-…-a-py’" class="headerlink" title="‘file‘: ‘D:\…\a.py’,"></a>‘<strong>file</strong>‘: ‘D:\…\a.py’,</h1><h1 id="‘test’"><a href="#‘test’" class="headerlink" title="‘test’: ,"></a>‘test’: <__main__.test object="" at="" 0x0000020f077e5da0="">,</__main__.test></h1><h1 id="‘clsfunc’"><a href="#‘clsfunc’" class="headerlink" title="‘clsfunc’: "></a>‘clsfunc’: <function test.func.<locals="">.inn_func at 0x0000020F071D7F28&gt;,</function></h1><h1 id="‘package‘-None"><a href="#‘package‘-None" class="headerlink" title="‘package‘: None,"></a>‘<strong>package</strong>‘: None,</h1><h1 id="‘name‘-‘main‘"><a href="#‘name‘-‘main‘" class="headerlink" title="‘name‘: ‘main‘,"></a>‘<strong>name</strong>‘: ‘<strong>main</strong>‘,</h1><h1 id="‘loader‘"><a href="#‘loader‘" class="headerlink" title="‘loader‘: "></a>‘<strong>loader</strong>‘: <_frozen_importlib_external.sourcefileloader object="" at="" 0x0000020f07289828=""></_frozen_importlib_external.sourcefileloader></h1><h1 id="-3"><a href="#-3" class="headerlink" title="}"></a>}</h1><p><strong>dict</strong></p>
<p>可写；以字典的形式返回命名空间所支持的任意自定义的函数属性。</p>
<p>print(‘Test.func.<strong>dict</strong>:’, Test.func.<strong>dict</strong>)</p>
<h1 id="Test-func-dict"><a href="#Test-func-dict" class="headerlink" title="Test.func.dict: {}"></a>Test.func.<strong>dict</strong>: {}</h1><p><strong>closure</strong></p>
<p>只读；以包含cell的元组形式返回闭包所包含的自由变量。</p>
<p>print(‘Test.func.<strong>closure</strong>:’, Test.func.<strong>closure</strong>)</p>
<h1 id="None"><a href="#None" class="headerlink" title="None"></a>None</h1><p>print(‘clsfunc.<strong>closure</strong>:’, clsfunc.<strong>closure</strong>)</p>
<h1 id="clsfunc-closure"><a href="#clsfunc-closure" class="headerlink" title="clsfunc.closure: ("></a>clsfunc.<strong>closure</strong>: (</h1><h1 id="-4"><a href="#-4" class="headerlink" title=","></a><cell at="" 0x0000020f071fe708:="" str="" object="" 0x0000020f07289998="">,</cell></h1><h1 id="-5"><a href="#-5" class="headerlink" title=""></a><cell at="" 0x0000020f072b8c78:="" str="" object="" 0x0000020f0766c538=""></cell></h1><h1 id="-6"><a href="#-6" class="headerlink" title=")"></a>)</h1><p>print(‘clsfunc.<strong>closure</strong>[x]:’, clsfunc.<strong>closure</strong>[0].cell_contents, clsfunc.<strong>closure</strong>[1].cell_contents)</p>
<h1 id="clsfunc-closure-x-dobi-dog"><a href="#clsfunc-closure-x-dobi-dog" class="headerlink" title="clsfunc.closure[x]: dobi dog"></a>clsfunc.<strong>closure</strong>[x]: dobi dog</h1><p><strong>annotations</strong></p>
<p>可写；具体详见“Python 的函数注释”(<a href="https://segmentfault.com/a/1190000005173184" target="_blank" rel="external">https://segmentfault.com/a/1190000005173184</a>)</p>
<p><strong>kwdefaults</strong></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/13/Python-自定义函数的特殊属性/" data-id="ciwps8inb000y50cya0x8s6sl" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/">&laquo; __('prev')</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/12/15/CentOS-6-x-Moodle/">CentOS 6.x Moodle</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-6-x-SmokePing/">CentOS 6.x SmokePing</a>
          </li>
        
          <li>
            <a href="/2016/12/15/1-刷固件/">1.刷固件</a>
          </li>
        
          <li>
            <a href="/2016/12/15/Linaro-12-11-Hadoop/">Linaro 12.11 Hadoop</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-6-x-CDH/">CentOS 6.x CDH</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 JinYan<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>