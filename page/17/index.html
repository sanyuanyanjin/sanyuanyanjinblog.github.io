<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SanYuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="SanYuan">
<meta property="og:url" content="http://yoursite.com/page/17/index.html">
<meta property="og:site_name" content="SanYuan">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SanYuan">
  
    <link rel="alternate" href="/atom.xml" title="SanYuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">SanYuan</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-医学影像信息系统" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/12/医学影像信息系统/" class="article-date">
  <time datetime="2016-12-12T06:28:24.000Z" itemprop="datePublished">2016-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/12/医学影像信息系统/">医学影像信息系统</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html" target="_blank" rel="external">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html</a><br><a href="http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/administration.html" target="_blank" rel="external">http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/administration.html</a><br>机架指定<br><a href="http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/cm_mc_specify_rack.html" target="_blank" rel="external">http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/cm_mc_specify_rack.html</a><br>hdfs nfs gateway<br><a href="http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/cm_mc_hdfs_nfs_gateway.html" target="_blank" rel="external">http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/cm_mc_hdfs_nfs_gateway.html</a><br>[root@n1 ~]# mount -t nfs -o vers=3,proto=tcp,nolock,noacl,sync n1:/  /media<br>[root@n1 ~]# df -h<br>Filesystem                  Size  Used Avail Use% Mounted on<br>/dev/mapper/vg_n1-LogVol00  1.8T  887G  839G  52% /<br>tmpfs                       7.9G   68K  7.9G   1% /dev/shm<br>/dev/sda1                   194M   35M  150M  19% /boot<br>cm_processes                7.9G   77M  7.8G   1% /opt/cm-5.1.3/run/cloudera-scm-agent/process<br>n1:/                         25T   12T   14T  46% /media<br>[root@n1 ~]#<br><a href="http://www.cnblogs.com/ZisZ/p/3437880.html" target="_blank" rel="external">http://www.cnblogs.com/ZisZ/p/3437880.html</a><br>HDFS Federation<br><a href="http://blog.csdn.net/strongerbit/article/details/7013221" target="_blank" rel="external">http://blog.csdn.net/strongerbit/article/details/7013221</a><br>HDFS Quotas<br><a href="http://blog.csdn.net/xichenguan/article/details/38679655" target="_blank" rel="external">http://blog.csdn.net/xichenguan/article/details/38679655</a><br>HDFS Snapshots<br><a href="http://blog.csdn.net/Androidlushangderen/article/details/51282612" target="_blank" rel="external">http://blog.csdn.net/Androidlushangderen/article/details/51282612</a><br>配置和使用 HBase<br><a href="http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/admin_hbase_config.html" target="_blank" rel="external">http://www.cloudera.com/content/www/zh-CN/documentation/enterprise/5-3-x/topics/admin_hbase_config.html</a><br>利用Hbase解决HDFS小文件合并<br><a href="http://blog.csdn.net/zhtzh312/article/details/49777463" target="_blank" rel="external">http://blog.csdn.net/zhtzh312/article/details/49777463</a></p>
<p>500M    /home/nfs/fir_fs_06/000/000/000/929<br>997M    /home/nfs/fir_fs_06/000/000/000/262<br>[root@n10 ~]# du -h –max-depth=1 /home/nfs/fir_fs_06/000/000/<br>3.0T /home/nfs/fir_fs_06/000/000/<br>279G /home/nfs/fir_fs_06/000/000/001<br>196G /home/nfs/fir_fs_06/000/000/000<br>166G /home/nfs/fir_fs_06/000/000/022<br>144G /home/nfs/fir_fs_06/000/000/002<br>120G /home/nfs/fir_fs_06/000/000/003<br>97G /home/nfs/fir_fs_06/000/000/021<br>74G /home/nfs/fir_fs_06/000/000/008<br>74G /home/nfs/fir_fs_06/000/000/004<br>70G /home/nfs/fir_fs_06/000/000/009<br>62G /home/nfs/fir_fs_06/000/000/013<br>62G /home/nfs/fir_fs_06/000/000/012<br>53G /home/nfs/fir_fs_06/000/000/019<br>53G /home/nfs/fir_fs_06/000/000/011<br>51G /home/nfs/fir_fs_06/000/000/007<br>44G /home/nfs/fir_fs_06/000/000/020<br>40G /home/nfs/fir_fs_06/000/000/014<br>40G /home/nfs/fir_fs_06/000/000/005<br>37G /home/nfs/fir_fs_06/000/000/018<br>34G /home/nfs/fir_fs_06/000/000/015<br>32G /home/nfs/fir_fs_06/000/000/010<br>31G /home/nfs/fir_fs_06/000/000/017<br>31G /home/nfs/fir_fs_06/000/000/006<br>29G /home/nfs/fir_fs_06/000/000/016<br>[root@n10 ~]#</p>
<p>sent 173878531328 bytes  received 8372059 bytes  5762328.41 bytes/sec<br>total size is 211310715474  speedup is 1.22<br>sending incremental file list</p>
<p>sent 5982406 bytes  received 16914 bytes  44275.42 bytes/sec<br>total size is 211310715474  speedup is 35222.44</p>
<p>[root@rsync ~]# sending incremental file list<br>000/<br>deleting 000/a</p>
<p>[root@rsync ~]#<br>sent 5982385 bytes  received 16917 bytes  72718.81 bytes/sec<br>total size is 211310715474  speedup is 35222.55<br>sending incremental file list<br>000/<br>deleting 000/1</p>
<p>[root@rsync ~]# </p>
<p>[root@n10 ~]# hadoop archive -archiveName tzuchi.har -p /tzuchi/tzuchi /output</p>
<p>[root@n10 ~]# hadoop fs -ls har:////output/tzuchi.har<br>Found 12 items<br>-rw-r–r–   3 root supergroup          0 2016-05-05 11:20 har:///output/tzuchi.har/1<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/ccnet<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/conf<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/installed<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/logs<br>drwxr-xr-x   - root supergroup          0 2016-05-04 17:06 har:///output/tzuchi.har/pids<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/seafile-data<br>drwxr-xr-x   - root supergroup          0 2016-05-04 17:06 har:///output/tzuchi.har/seafile-server-5.1.1<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:59 har:///output/tzuchi.har/seafile-server-latest<br>-rw-r–r–   3 root supergroup   21443019 2016-05-05 13:13 har:///output/tzuchi.har/seafile-server_5.1.1_x86-64.tar.gz<br>drwxr-xr-x   - root supergroup          0 2016-05-04 16:52 har:///output/tzuchi.har/seahub-data<br>drwxr-xr-x   - root supergroup          0 2016-05-05 10:31 har:///output/tzuchi.har/tzuchi</p>
<p>hadoop fs –get hdfs://namenode/foo/file-1 localdir</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/12/医学影像信息系统/" data-id="ciwvcf4w7003e8icy5l9z89m6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Comparison-of-8-kinds-of-Nosql-database-system" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/12/Comparison-of-8-kinds-of-Nosql-database-system/" class="article-date">
  <time datetime="2016-12-12T06:19:22.000Z" itemprop="datePublished">2016-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/12/Comparison-of-8-kinds-of-Nosql-database-system/">Comparison of 8 kinds of Nosql database system</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li>CouchDB</li>
</ol>
<p>所用语言： Erlang<br>特点：DB一致性，易于使用<br>使用许可： Apache<br>协议： HTTP/REST<br>双向数据复制，<br>持续进行或临时处理，<br>处理时带冲突检查，<br>因此，采用的是master-master复制（见编注2）<br>MVCC – 写操作不阻塞读操作<br>可保存文件之前的版本<br>Crash-only（可靠的）设计<br>需要不时地进行数据压缩<br>视图：嵌入式 映射/减少<br>格式化视图：列表显示<br>支持进行服务器端文档验证<br>支持认证<br>根据变化实时更新<br>支持附件处理<br>因此， CouchApps（独立的 js应用程序）<br>需要 jQuery程序库</p>
<p>最佳应用场景：适用于数据变化较少，执行预定义查询，进行数据统计的应用程序。适用于需要提供数据版本支持的应用程序。</p>
<p>例如： CRM、CMS系统。 master-master复制对于多站点部署是非常有用的。</p>
<p>（编注2：master-master复制：是一种数据库同步方法，允许数据在一组计算机之间共享数据，并且可以通过小组中任意成员在组内进行数据更新。）</p>
<ol>
<li>Redis</li>
</ol>
<p>所用语言：C/C++<br>特点：运行异常快<br>使用许可： BSD<br>协议：类 Telnet<br>有硬盘存储支持的内存数据库，<br>但自2.0版本以后可以将数据交换到硬盘（注意， 2.4以后版本不支持该特性！）<br>Master-slave复制（见编注3）<br>虽然采用简单数据或以键值索引的哈希表，但也支持复杂操作，例如 ZREVRANGEBYSCORE。<br>INCR &amp; co （适合计算极限值或统计数据）<br>支持 sets（同时也支持 union/diff/inter）<br>支持列表（同时也支持队列；阻塞式 pop操作）<br>支持哈希表（带有多个域的对象）<br>支持排序 sets（高得分表，适用于范围查询）<br>Redis支持事务<br>支持将数据设置成过期数据（类似快速缓冲区设计）<br>Pub/Sub允许用户实现消息机制</p>
<p>最佳应用场景：适用于数据变化快且数据库大小可遇见（适合内存容量）的应用程序。</p>
<p>例如：股票价格、数据分析、实时数据搜集、实时通讯。</p>
<p>（编注3：Master-slave复制：如果同一时刻只有一台服务器处理所有的复制请求，这被称为 Master-slave复制，通常应用在需要提供高可用性的服务器集群。）</p>
<ol>
<li>MongoDB</li>
</ol>
<p>所用语言：C++<br>特点：保留了SQL一些友好的特性（查询，索引）。<br>使用许可： AGPL（发起者： Apache）<br>协议： Custom, binary（ BSON）<br>Master/slave复制（支持自动错误恢复，使用 sets 复制）<br>内建分片机制<br>支持 javascript表达式查询<br>可在服务器端执行任意的 javascript函数<br>update-in-place支持比CouchDB更好<br>在数据存储时采用内存到文件映射<br>对性能的关注超过对功能的要求<br>建议最好打开日志功能（参数 –journal）<br>在32位操作系统上，数据库大小限制在约2.5Gb<br>空数据库大约占 192Mb<br>采用 GridFS存储大数据或元数据（不是真正的文件系统）</p>
<p>最佳应用场景：适用于需要动态查询支持；需要使用索引而不是 map/reduce功能；需要对大数据库有性能要求；需要使用 CouchDB但因为数据改变太频繁而占满内存的应用程序。</p>
<p>例如：你本打算采用 MySQL或 PostgreSQL，但因为它们本身自带的预定义栏让你望而却步。</p>
<ol>
<li>Riak</li>
</ol>
<p>所用语言：Erlang和C，以及一些Javascript<br>特点：具备容错能力<br>使用许可： Apache<br>协议： HTTP/REST或者 custom binary<br>可调节的分发及复制(N, R, W)<br>用 JavaScript or Erlang在操作前或操作后进行验证和安全支持。<br>使用JavaScript或Erlang进行 Map/reduce<br>连接及连接遍历：可作为图形数据库使用<br>索引：输入元数据进行搜索（1.0版本即将支持）<br>大数据对象支持（ Luwak）<br>提供“开源”和“企业”两个版本<br>全文本搜索，索引，通过 Riak搜索服务器查询（ beta版）<br>支持Masterless多站点复制及商业许可的 SNMP监控</p>
<p>最佳应用场景：适用于想使用类似 Cassandra（类似Dynamo）数据库但无法处理 bloat及复杂性的情况。适用于你打算做多站点复制，但又需要对单个站点的扩展性，可用性及出错处理有要求的情况。</p>
<p>例如：销售数据搜集，工厂控制系统；对宕机时间有严格要求；可以作为易于更新的 web服务器使用。</p>
<ol>
<li>Membase</li>
</ol>
<p>所用语言： Erlang和C<br>特点：兼容 Memcache，但同时兼具持久化和支持集群<br>使用许可： Apache 2.0<br>协议：分布式缓存及扩展<br>非常快速（200k+/秒），通过键值索引数据<br>可持久化存储到硬盘<br>所有节点都是唯一的（ master-master复制）<br>在内存中同样支持类似分布式缓存的缓存单元<br>写数据时通过去除重复数据来减少 IO<br>提供非常好的集群管理 web界面<br>更新软件时软无需停止数据库服务<br>支持连接池和多路复用的连接代理</p>
<p>最佳应用场景：适用于需要低延迟数据访问，高并发支持以及高可用性的应用程序</p>
<p>例如：低延迟数据访问比如以广告为目标的应用，高并发的 web 应用比如网络游戏（例如 Zynga）</p>
<ol>
<li>Neo4j</li>
</ol>
<p>所用语言： Java<br>特点：基于关系的图形数据库<br>使用许可： GPL，其中一些特性使用 AGPL/商业许可<br>协议： HTTP/REST（或嵌入在 Java中）<br>可独立使用或嵌入到 Java应用程序<br>图形的节点和边都可以带有元数据<br>很好的自带web管理功能<br>使用多种算法支持路径搜索<br>使用键值和关系进行索引<br>为读操作进行优化<br>支持事务（用 Java api）<br>使用 Gremlin图形遍历语言<br>支持 Groovy脚本<br>支持在线备份，高级监控及高可靠性支持使用 AGPL/商业许可</p>
<p>最佳应用场景：适用于图形一类数据。这是 Neo4j与其他nosql数据库的最显著区别</p>
<p>例如：社会关系，公共交通网络，地图及网络拓谱</p>
<ol>
<li>Cassandra</li>
</ol>
<p>所用语言： Java<br>特点：对大型表格和 Dynamo支持得最好<br>使用许可： Apache<br>协议： Custom, binary (节约型)<br>可调节的分发及复制(N, R, W)<br>支持以某个范围的键值通过列查询<br>类似大表格的功能：列，某个特性的列集合<br>写操作比读操作更快<br>基于 Apache分布式平台尽可能地 Map/reduce<br>我承认对 Cassandra有偏见，一部分是因为它本身的臃肿和复杂性，也因为 Java的问题（配置，出现异常，等等）</p>
<p>最佳应用场景：当使用写操作多过读操作（记录日志）如果每个系统组建都必须用 Java编写（没有人因为选用 Apache的软件被解雇）</p>
<p>例如：银行业，金融业（虽然对于金融交易不是必须的，但这些产业对数据库的要求会比它们更大）写比读更快，所以一个自然的特性就是实时数据分析</p>
<ol>
<li>HBase</li>
</ol>
<p>（配合 ghshephard使用）</p>
<p>所用语言： Java<br>特点：支持数十亿行X上百万列<br>使用许可： Apache<br>协议：HTTP/REST （支持 Thrift，见编注4）<br>在 BigTable之后建模<br>采用分布式架构 Map/reduce<br>对实时查询进行优化<br>高性能 Thrift网关<br>通过在server端扫描及过滤实现对查询操作预判<br>支持 XML, Protobuf, 和binary的HTTP<br>Cascading, hive, and pig source and sink modules<br>基于 Jruby（ JIRB）的shell<br>对配置改变和较小的升级都会重新回滚<br>不会出现单点故障<br>堪比MySQL的随机访问性能</p>
<p>最佳应用场景：适用于偏好BigTable:)并且需要对大数据进行随机、实时访问的场合。</p>
<p>例如： Facebook消息数据库（更多通用的用例即将出现）</p>
<p>编注4：Thrift 是一种接口定义语言，为多种其他语言提供定义和创建服务，由Facebook开发并开源。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/12/Comparison-of-8-kinds-of-Nosql-database-system/" data-id="ciwvcf4un00148icylilus80k" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Centos-6-5-x64-Hadoop-2-3-0" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/12/Centos-6-5-x64-Hadoop-2-3-0/" class="article-date">
  <time datetime="2016-12-12T06:14:14.000Z" itemprop="datePublished">2016-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/12/Centos-6-5-x64-Hadoop-2-3-0/">Centos 6.5 x64 Hadoop-2.3.0</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>关闭防火墙：<br>service iptables stop （临时关闭）<br>chkconfig iptables off （重启后生效）</p>
<p>关闭SELINUX<br>vim /etc/selinux/config<br>SELINUX=disabled</p>
<p>groupadd hadoop<br>useradd -g hadoop hadoop</p>
<p>配置SSH<br>yum -y install openssh-server<br>vim /etc/ssh/sshd_config<br>PermitRootLogin yes<br>service sshd restart<br>yum install openssh-clients -y</p>
<p>无密码登录<br>root用户<br>[root@n0 ~]# ssh-keygen -t rsa -P “”<br>[root@n0 ~]# cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys<br>[root@n0 ~]# ssh localhost</p>
<p>普通用户<br>/home/hadoop/.ssh<br>[hadoop@n0 .ssh]$ ssh-keygen -t rsa -P “”<br>[hadoop@n0 .ssh]$ cat id_rsa.pub &gt;&gt; authorized_keys<br>[hadoop@n0 .ssh]$ chmod 600 authorized_keys<br>[hadoop@n0 .ssh]$ ssh localhost</p>
<p>安装配置VNC<br>设置密码<br>yum -y install tigervnc-server<br>vncpasswd<br>更改配置<br>vim /etc/sysconfig/vncservers<br>VNCSERVERS=”0:hadoop”<br>VNCSERVERARGS[0]=”-geometry 1024x768”</p>
<p>service vncserver restart<br>开机启动<br>chkconfig –level 345 vncserver on</p>
<p>卸载内置Java<br>rpm -qa | grep java<br>rpm -e –nodeps java-1.7.0-openjdk-1.7.0.45-2.4.3.3.el6.x86_64<br>rpm -e –nodeps java-1.6.0-openjdk-1.6.0.0-1.66.1.13.0.el6.x86_64<br>rpm -e –nodeps tzdata-java-2013g-1.el6.noarch</p>
<p>[root@n0 ~]# tar -zxvf jdk-7u79-linux-x64.tar.gz<br>[root@n0 ~]# mv jdk1.7.0_79/ /usr/lib/java<br>[root@n0 ~]# vim /etc/profile<br>export JAVA_HOME=/usr/lib/java/jdk1.7.0_79/<br>export JRE_HOME=${JAVA_HOME}/jre<br>export CLASS_PATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib<br>export PATH=$PATH:${JAVA_HOME}/bin</p>
<p>export HADOOP_HOME=/usr/local/hadoop/<br>export PATH=$PATH:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin<br>export HADOOP_MAPRED_HOME=${HADOOP_HOME}<br>export HADOOP_COMMON_HOME=${HADOOP_HOME}<br>export HADOOP_HDFS_HOME=${HADOOP_HOME}<br>export YARN_HOME=${HADOOP_HOMEL}<br>export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/natvie<br>export HADOOP_OPTS=”-Djava.library.path=${HADOOP_HOME}/lib:${HADOOP_HOME}/lib/native”</p>
<p>[root@n0 ~]# source /etc/profile<br>[root@n0 ~]# java -version<br>java version “1.7.0_79”<br>Java(TM) SE Runtime Environment (build 1.7.0_79-b15)<br>Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)</p>
<p>[root@n0 ~]# tar -zxvf hadoop-2.3.0-x64.tar.gz<br>[root@n0 ~]# mv hadoop-2.3.0 /usr/local/hadoop<br>[root@n0 ~]# chmod -R 755 /usr/local/hadoop/<br>[root@n0 ~]# chown -R hadoop:hadoop /usr/local/hadoop/<br>[root@n0 ~]# ll /usr/local/hadoop/<br>/usr/local/hadoop/etc/hadoop<br>vim core-site.xml<br>    <property><br>        <name>hadoop.tmp.dir</name><br>        <value>/usr/local/hadoop/tmp</value><br>    </property><br>    <property><br>        <name>fs.defaultFS</name><br>        <value>hdfs://localhost:9000</value><br>     </property></p>
<p>vim hdfs-site.xml</p>
<property><br>        <name>dfs.replication</name><br>        <value>1</value><br>    </property><br>    <property><br>        <name>dfs.namenode.name.dir</name><br>        <value>file:/usr/local/hadoop/dfs/name</value><br>    </property><br>    <property><br>        <name>dfs.datanode.data.dir</name><br>        <value>file:/usr/local/hadoop/dfs/data</value><br>    </property><br>    <property><br>            <name>dfs.permissions</name><br>            <value>false</value><br>     </property>

<p>vim yarn-site.xml</p>
<property><br><name>mapreduce.framework.name</name><br><value>yarn</value><br></property>

<property><br><name>yarn.nodemanager.aux-services</name><br><value>mapreduce_shuffle</value><br></property>

<property><br><name>yarn.resourcemanager.scheduler.address</name><br><value>localhost:9001</value><br></property>

<p>[hadoop@n0 hadoop]$ cd /usr/local/hadoop/<br>[hadoop@n0 hadoop]$ mkdir tmp dfs dfs/name dfs/data<br>[hadoop@n0 hadoop]$ ll</p>
<p>[hadoop@n0 ~]$ hdfs namenode -format<br>15/10/29 14:52:56 INFO common.Storage: Storage directory /usr/local/hadoop/dfs/name has been successfully formatted.<br>[hadoop@n0 ~]$ start-dfs.sh<br>Starting namenodes on [localhost]<br>localhost: starting namenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-namenode-n0.out<br>localhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-hadoop-datanode-n0.out<br>Starting secondary namenodes [0.0.0.0]<br>The authenticity of host ‘0.0.0.0 (0.0.0.0)’ can’t be established.<br>RSA key fingerprint is a4:f8:d8:ba:42:46:7a:f3:5d:82:7a:4e:e3:08:17:df.<br>Are you sure you want to continue connecting (yes/no)? yes<br>0.0.0.0: Warning: Permanently added ‘0.0.0.0’ (RSA) to the list of known hosts.<br>0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-hadoop-secondarynamenode-n0.out<br>[hadoop@n0 ~]$ start-yarn.sh<br>starting yarn daemons<br>starting resourcemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-resourcemanager-n0.out<br>localhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-hadoop-nodemanager-n0.out<br>[hadoop@n0 ~]$ jps<br>12108 SecondaryNameNode<br>12742 Jps<br>11932 DataNode<br>11840 NameNode<br>12313 ResourceManager<br>12408 NodeManager</p>
<p>[hadoop@n0 hadoop]$ hadoop fs -mkdir -p /data/wordcount<br>[hadoop@n0 hadoop]$ hadoop fs -mkdir -p /output/<br>[hadoop@n0 hadoop]$ hadoop fs -put *.xml /data/wordcount<br>[hadoop@n0 hadoop]$ hadoop fs -ls /data/wordcount<br>[hadoop@n0 hadoop]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.3.0.jar wordcount /data/wordcount /output/wordcount<br>15/10/29 15:48:00 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032<br>15/10/29 15:48:01 INFO input.FileInputFormat: Total input paths to process : 7<br>15/10/29 15:48:01 INFO mapreduce.JobSubmitter: number of splits:7<br>15/10/29 15:48:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1446104470000_0001<br>15/10/29 15:48:03 INFO impl.YarnClientImpl: Submitted application application_1446104470000_0001<br>15/10/29 15:48:03 INFO mapreduce.Job: The url to track the job: <a href="http://n0:8088/proxy/application_1446104470000_0001/" target="_blank" rel="external">http://n0:8088/proxy/application_1446104470000_0001/</a><br>15/10/29 15:48:03 INFO mapreduce.Job: Running job: job_1446104470000_0001<br>15/10/29 15:48:16 INFO mapreduce.Job: Job job_1446104470000_0001 running in uber mode : false<br>15/10/29 15:48:16 INFO mapreduce.Job:  map 0% reduce 0%<br>15/10/29 15:48:29 INFO mapreduce.Job:  map 86% reduce 0%<br>15/10/29 15:48:36 INFO mapreduce.Job:  map 100% reduce 0%<br>15/10/29 15:48:38 INFO mapreduce.Job:  map 100% reduce 100%<br>15/10/29 15:48:39 INFO mapreduce.Job: Job job_1446104470000_0001 completed successfully<br>15/10/29 15:48:39 INFO mapreduce.Job: Counters: 49</p>
<p>[hadoop@n0 hadoop]$ hadoop fs -cat /output/wordcount/part-r-00000 |head<br>“*”     17<br>“AS     7<br>“License”);     7<br>“alice,bob      17<br>(ASF)   1<br>(root   1<br>(the    7<br>–&gt;     13<br>0.0     1<br>1.0.    1</p>
<p>[hadoop@n0 hadoop]$ cp core-site.xml hdfs-site.xml /home/hadoop/workspace/HDFSTest/bin/<br>[hadoop@n0 hadoop]$ cp core-site.xml hdfs-site.xml /home/hadoop/workspace/MapreduceTest/bin/</p>
<p>[hadoop@n0 ~]$ hadoop archive -archiveName test.har -p /data /data/har<br>[hadoop@n0 ~]$ hadoop fs -ls har:///data/har/test.har</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/12/Centos-6-5-x64-Hadoop-2-3-0/" data-id="ciwvcf4uj000x8icy2iieif1z" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-test" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/12/test/" class="article-date">
  <time datetime="2016-12-12T06:03:19.000Z" itemprop="datePublished">2016-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/12/12/test/">test</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/12/test/" data-id="ciwvcf4vr002h8icyd8ht9czu" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <a class="extend prev" rel="prev" href="/page/16/">&laquo; __('prev')</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="page-number" href="/page/16/">16</a><span class="page-number current">17</span>
  </nav>
</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/12/15/NGINX日志切割/">NGINX日志切割</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-6-x-安装SVN/">CentOS 6.x 安装SVN</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-7-x-git/">CentOS 7.x git</a>
          </li>
        
          <li>
            <a href="/2016/12/15/开源资源/">开源资源</a>
          </li>
        
          <li>
            <a href="/2016/12/15/开源软件/">开源软件</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 JinYan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>