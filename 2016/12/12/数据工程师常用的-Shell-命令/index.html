<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>数据工程师常用的 Shell 命令 | SanYuan</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Linux以其强大的命令行称霸江湖，Shell命令是数据极客的必修兵器。探索性数据分析，在需求和数据都不太明确的环境下，使用各种命令进行一次探索与挖掘。从基础的文件查看到简单的统计，再到一些常用的探索性分析命令，其目的都只是为了更好的做数据分析与挖掘而已。01 Shell命令行
对于经常和数据打交道的人来说，数据工程师应该也是常常和Linux打交道。Linux以其强大的命令行称霸江湖，因此，She">
<meta property="og:type" content="article">
<meta property="og:title" content="数据工程师常用的 Shell 命令">
<meta property="og:url" content="http://yoursite.com/2016/12/12/数据工程师常用的-Shell-命令/index.html">
<meta property="og:site_name" content="SanYuan">
<meta property="og:description" content="Linux以其强大的命令行称霸江湖，Shell命令是数据极客的必修兵器。探索性数据分析，在需求和数据都不太明确的环境下，使用各种命令进行一次探索与挖掘。从基础的文件查看到简单的统计，再到一些常用的探索性分析命令，其目的都只是为了更好的做数据分析与挖掘而已。01 Shell命令行
对于经常和数据打交道的人来说，数据工程师应该也是常常和Linux打交道。Linux以其强大的命令行称霸江湖，因此，She">
<meta property="og:updated_time" content="2016-12-12T09:07:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据工程师常用的 Shell 命令">
<meta name="twitter:description" content="Linux以其强大的命令行称霸江湖，Shell命令是数据极客的必修兵器。探索性数据分析，在需求和数据都不太明确的环境下，使用各种命令进行一次探索与挖掘。从基础的文件查看到简单的统计，再到一些常用的探索性分析命令，其目的都只是为了更好的做数据分析与挖掘而已。01 Shell命令行
对于经常和数据打交道的人来说，数据工程师应该也是常常和Linux打交道。Linux以其强大的命令行称霸江湖，因此，She">
  
    <link rel="alternate" href="/atom.xml" title="SanYuan" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">SanYuan</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-数据工程师常用的-Shell-命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/12/12/数据工程师常用的-Shell-命令/" class="article-date">
  <time datetime="2016-12-12T09:04:24.000Z" itemprop="datePublished">2016-12-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      数据工程师常用的 Shell 命令
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Linux以其强大的命令行称霸江湖，Shell命令是数据极客的必修兵器。探索性数据分析，在需求和数据都不太明确的环境下，使用各种命令进行一次探索与挖掘。从基础的文件查看到简单的统计，再到一些常用的探索性分析命令，其目的都只是为了更好的做数据分析与挖掘而已。<br>01 Shell命令行</p>
<p>对于经常和数据打交道的人来说，数据工程师应该也是常常和Linux打交道。Linux以其强大的命令行称霸江湖，因此，Shell命令也是数据极客的必修兵器。</p>
<p>利用Linux命令行的几个命令，就可以完成一些简单的统计分析工作，比如利用wc命令统计文件行，单词数，字符数，利用sort排序和去重，再结合uniq可以进行词频统计。比如：<br>$ cat file.txt<br>yunjie<br>yunjie-talk<br>yunjie-yun<br>yunjie<br>yunjie-shuo<br>$ sort file.txt | uniq -c | sort -nr | head -5<br>   2 yunjie<br>   1 yunjie-shuo<br>   1 yunjie-talk<br>   1 yunjie-yun</p>
<p>先用cat命令，了解一下文件的大概格式与内容，发现每行为一个单词。现在需要统计这些单词出现的频率，以及显示出现次数最多的5个单词。</p>
<p>先对文件进行排序，这样相同的单词在紧挨着的行，再后uniq -c 命令，统计不同的单词及各个单词出现的次数。这样得到的结果就是次数后面紧接着单词，然后使用sort -nr对次数进行排序，并逆序显示，最后head命令显示结果的前5行。</p>
<p>非常简单的一种方式，读取文件，排序，统计，再对统计结果进行逆序，最后只显示前几个结果。</p>
<p>类似于sql语句：</p>
<p>select word,count(1) cnt<br>from file<br>group by word<br>order by cnt desc<br>limit 5;</p>
<p>如果对sql语句熟悉的话，上面的形式应该更容易理解。虽然实现的思想和方式非常简单，但在实际的探索性数据分析中使用却非常频繁。</p>
<p>02 探索性分析</p>
<p>比如在日志分析中，有时并没有非常明确的目标，或者即使有明确的目标，通常各种数据也并没有明确的定义。比如，别人丢给你一个压缩文件，说想分析一下里面有哪些是异常的访问请求。任务描述就是这样，没有更明确的了。</p>
<p>拿到日志文件和这样的分析任务，就需要进行各种可能的探索性分析。先看一下文件的格式，是否压缩过，使用gzip压缩还是tar压缩。解压后，需要先大概了解一下，文件是什么样的格式。对于网络请求的日志文件，是一行一个请求和响应，还是多行一个请求和响应。查看文件有多少行，查看文件占用空间大小。如果解压后包含多个目录或者文件，同样的一个命令，更能发挥强大效果。此时，通常需要如下命令：</p>
<p>gzip/tar：压缩/解压<br>cat/zcat：文件查看<br>less/more：文件查看，支持gz压缩格式直接查看<br>head/tail：查看文件前/后10行<br>wc：统计行数、单词数、字符数<br>du -h -c -s：查看空间占用</p>
<p>上面有一个比较有趣的命令组，less和more，这两个都可以分页查看文件。最开始有的more命令，好像是当时more不支持向后翻页。于是一帮人就在此基础上进行了改进，直接叫less，和more同样的功能只是更强大些。因此，也发展出了“less is more”的哲学，“少即是多”，而且少比多更好。这种思想，在产品设计与代码优化中都有体现。</p>
<p>了解文件的大概信息后，可能需要提取一行中某个字段的内容，或者需要搜索某些行出来，或者需要对某些字符或者行进行一定的修改操作，或者需要在众多的目录和文件中找出某此天的日志（甚至找到后需要对这些天的日志进行统一处理），此时下面这些命令可以帮你：</p>
<p>awk：命令行下的数据库操作工具<br>join/cut/paste：关联文件/切分字段/合并文件<br>fgrep/grep/egrep：全局正则表达式查找<br>find：查找文件，并且对查找结果批量化执行任务<br>sed：流编辑器，批量修改、替换文件<br>split：对大文件进行切分处理，按多少行一个文件，或者多少字节一个文件<br>rename：批量重命名(Ubuntu上带的perl脚本，其它系统需要安装)，使用-n命令进行测试</p>
<p>如：</p>
<h1 id="解压缩日志"><a href="#解压缩日志" class="headerlink" title="解压缩日志"></a>解压缩日志</h1><p>$ gzip -d a.gz<br>$ tar zcvf/jcvf one.tar.bz2 one</p>
<h1 id="直接查看压缩日志"><a href="#直接查看压缩日志" class="headerlink" title="直接查看压缩日志"></a>直接查看压缩日志</h1><p>$ less a.gz  </p>
<h1 id="无需先解压"><a href="#无需先解压" class="headerlink" title="无需先解压"></a>无需先解压</h1><p>另外，以z开头的几个命令可以简单处理gzip压缩文件, 如zcat：直接打印压缩文件，还有zgrep/zfgrep/zegrep，在压缩文件中直接查找。</p>
<h1 id="查询字符串，并显示匹配行的前3行和后3行内容"><a href="#查询字符串，并显示匹配行的前3行和后3行内容" class="headerlink" title="查询字符串，并显示匹配行的前3行和后3行内容"></a>查询字符串，并显示匹配行的前3行和后3行内容</h1><p>fgrep ‘yunjie-talk’ -A 3 -B 3 log.txt</p>
<h1 id="在当前目前-及子目录-下，所有的log文件中搜索字符串hacked-by"><a href="#在当前目前-及子目录-下，所有的log文件中搜索字符串hacked-by" class="headerlink" title="在当前目前(及子目录)下，所有的log文件中搜索字符串hacked by:"></a>在当前目前(及子目录)下，所有的log文件中搜索字符串hacked by:</h1><p>$ find . -name “*.log” | xargs fgrep “hacked by”</p>
<p>fgrep, grep, egrep的一些区别：</p>
<p>fgrep按字符串的本来意思完全匹配，里面的正则元字符当成普通字符解析， 如： fgrep “1.2.3.4″ 则只匹配ip地址： 1.2.3.4, 其中的.不会匹配任意字符。fgrep当然会比grep快多了。写起来又简单，不用转义。<br>grep只使用普通的一些正则，egrep或者grep -E使用扩展的正则，如</p>
<p>egrep “one|two”, 匹配one或者two<br>grep -E -v “.jpg|.png|.gif|.css|.js” log.txt |wc -l</p>
<p>查找所有来自日本的ip的请求，先把所有来源ip取出来，去重，找出日本的ip，放入文件japan.ip，再使用命令：</p>
<p>$ cat log.gz | gzip -d | fgrep -f japan.ip &gt; japan.log</p>
<p>对hive中导出的文件，替换01</p>
<p>cat 0000* | sed ‘s/x1/ /g’ &gt; log.txt<br>03 其它常用命令</p>
<p>如果文件编码是从windows上传过来的gb2312编码，需要处理成utf8的编码，或者某个日志被黑客后来修改过了，需要和原来的备份数据进行对比，这些工作都是需要数据工程师自己能熟悉的掌握。</p>
<p>假如日志文件是最近一年的请求日志，那么可能是按天或者按小时进行单独存放，此时如果只需要提取某些天（比如周末）的数据，很可能需要处理时间。</p>
<p>因此，下面的一些命令或者工具就很有用了：</p>
<p>date：命令行时间操作函数<br>sort/uniq：排序、去重、统计<br>comm：对两个排序文件进行按行比较（共同行、只出现在左边文件、只出现在右边文件）<br>diff：逐字符比较文件的异同，配合cdiff，类似于github的显示效果<br>curl/w3m/httpie：命令行下进行网络请求<br>iconv：文件编码转换，如：iconv -f GB2312 -t UTF8 1.csv &gt; 2.csv<br>seq：产生连续的序列，配合for循环使用</p>
<p>输出今天/昨天的日期字符串</p>
<p>$ date -d today +%Y%m%d<br>20160320<br>$ date -d yesterday +%Y%m%d<br>20160319</p>
<p>对unix秒的处理</p>
<h1 id="当前的时间"><a href="#当前的时间" class="headerlink" title="当前的时间"></a>当前的时间</h1><p>$ date +%s<br>1458484275<br>$date -d @1458484275<br>Sun Mar 20 22:31:15 CST 2016</p>
<p>两个文件a.txt, b.txt求只出现在a.txt中的数据：</p>
<h1 id="排序两个文件"><a href="#排序两个文件" class="headerlink" title="排序两个文件"></a>排序两个文件</h1><p>$ sort a.txt &gt; a.txt.sort<br>$ sort b.txt &gt; b.txt.sort</p>
<h1 id="求只出现在c-sh中的内容"><a href="#求只出现在c-sh中的内容" class="headerlink" title="求只出现在c.sh中的内容"></a>求只出现在c.sh中的内容</h1><p>$ comm -2 -3 a.txt.sort b.txt.sort</p>
<p>04 批量操作</p>
<p>对上面的文件进行了一番探索分析后，可能已经有一定的线索或者眉目了，需要更进一步的处理大量的文件或者字段了。此时的步骤也许是一个消耗时间的过程，也许是一个需要看缘分的过程。总之，可能需要综合上面的一些命令，并且对大量的日志进行处理。</p>
<p>这也是体现Shell更强大的一面——批量化的功能了。命令比图形界面的最大优势就是，只需熟悉了，就很容易实现批量化操作，将这些批量化的命令组合成一个文件，于是便产生了脚本。</p>
<p>批量化命令或者脚本，熟悉几个常用的流程控制，就能发挥出强大的性能：</p>
<p>if条件判断：</p>
<p>if [ -d ${base_d} ];<br>    then mkdir -p ${base_d};<br>fi</p>
<p>while循环：</p>
<p>while<br>do<br>    do_something;<br>done</p>
<p>for循环（用得很多）：</p>
<p>for x in *.log.gz;<br>do<br>    gzip -d ${x};<br>done</p>
<p>这几个条件判断与循环，也可以直接在命令行下使用，区别是多加几个分号隔开即可。</p>
<p>另外，执行长时间的任务，最好直接用nohup来操作。</p>
<p>生成过去8天的日期序列：</p>
<p>$for num in <code>seq 8 -1 1</code>;do dd=<code>date --date=&quot;${num} day ago&quot; +%Y%m%d</code>;echo ${dd};done<br>20160312<br>20160313<br>20160314<br>20160315<br>20160316<br>20160317<br>20160318<br>20160319</p>
<p>有目录和文件如下：</p>
<p>20160320 目录<br>    10.1.0.1_20160320<em>.log.gz   目录<br>        201603200000.log.gz          文件<br>        201603200010.log.gz          文件<br>    10.1.0.2_20160320</em>.log.gz   目录<br>        201603200000.log.gz         文件<br>        201603200010.log.gz         文件</p>
<p>需求：去掉目录中的*.log.gz，这样很容易让人误解为文件。 rename -n为测试，rename使用和sed相同的语法。</p>
<p>$ for d in 201603??;do echo ${d}; cd ${d}; rename -n ‘s/<em>.log.gz//‘ </em>.log.gz ; cd ..;done</p>
<p>测试完成后，使用rename不加-n为真正执行重命名操作。</p>
<p>05 结尾</p>
<p>这儿只是简单列举了一些数据分析或者数据处理相关的命令，只能算是Linux的Shell那博大精深的命令中的冰山一角。</p>
<p>但如果能把这些相关的命令融会贯通，并且能实际使用的话，也算是在数据极客之路上多走了一步。</p>
<p>从基础的文件查看到简单的统计，再到一些常用的探索性分析命令，其目的都只是为了更好的做数据分析与挖掘而已。能综合这些命令，并组合起来使用，将命令存放到文件，即产生了Shell脚本。Shell脚本本身也是一门强大的学问了，其中各个命令还有每个命令支持的参数，值得慢慢研究。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/12/12/数据工程师常用的-Shell-命令/" data-id="ciwvcpukw003zoucyrgakr8le" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2016/12/12/如何用十条命令在一分钟内检查Linux服务器性能/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          如何用十条命令在一分钟内检查Linux服务器性能
        
      </div>
    </a>
  
  
    <a href="/2016/12/12/awk/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">awk</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/12/">December 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/12/19/书单/">书单</a>
          </li>
        
          <li>
            <a href="/2016/12/15/NGINX日志切割/">NGINX日志切割</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-6-x-安装SVN/">CentOS 6.x 安装SVN</a>
          </li>
        
          <li>
            <a href="/2016/12/15/CentOS-7-x-git/">CentOS 7.x git</a>
          </li>
        
          <li>
            <a href="/2016/12/15/开源资源/">开源资源</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 JinYan<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>